{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Construindo um LLM do Zero: criando significado com Embeddings\n",
    "\n",
    "Este é o **terceiro** de uma série de oito artigos que podem ser encontrados no meu medium. Acesse o primeiro artigo da série aqui: [Construindo um LLM: entendendo os Grandes Modelos de Linguagem](https://blog.zfab.me/construindo-um-llm-entendendo-os-grandes-modelos-de-linguagem-b37884219eaa)\n",
    "\n",
    "--\n",
    "\n",
    "Você sabia que as palavras podem ser organizadas em um espaço multidimensional que revela suas relações semânticas e contextuais?\n",
    "\n",
    "Imagine uma biblioteca onde nenhum livro possui título ou categoria visível. Para organizá-la, analisamos características como peso, número de páginas e autor, posicionando os livros nas prateleiras de acordo com suas similaridades. É exatamente isso que **embeddings** fazem, transformam palavras, frases ou tokens em representações dentro de um espaço multidimensional — e é esse universo que vamos explorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Representação Vetorial dos Tokens**\n",
    "\n",
    "Anteriormente, através do processo de tokenização e do vocabulário, convertemos palavras em identificadores numéricos únicos (IDs). Porém, usar esses IDs diretamente como entrada não é eficaz. Por exemplo, em um vocabulário onde \"gato\" tem ID 1, \"elefante\" ID 2 e \"cachorro\" ID 3, o modelo poderia interpretar erroneamente esses números como valores ordinais, sugerindo que \"cachorro\" (ID 3) é maior ou tem uma relação quantitativa com \"elefante\" (ID 2).\n",
    "\n",
    "Os IDs são identificadores arbitrários que, sozinhos, não carregam o verdadeiro significado dos tokens que representam. É como um CPF: ele não representa quem você é, apenas serve como um identificador numérico. Para informar melhor ao modelo o que cada ID significa, utilizamos vetores – listas de números – que nos oferecem maior flexibilidade na representação dos tokens. Em vez de um único número, trabalharemos com uma lista deles.\n",
    "\n",
    "Surge então um novo desafio: como criar vetores que não sejam apenas listas arbitrárias de números, mas que realmente transmitam o significado das palavras? Para resolver essa questão, foram desenvolvidas várias técnicas que transformam palavras em representações numéricas mais ricas e significativas.\n",
    "\n",
    "Uma abordagem fundamental para transformar tokens em vetores é conhecida como **one-hot encoding**. Esta técnica, embora simples, ilustra bem o conceito de representação vetorial: cada palavra é convertida em um vetor binário onde apenas a posição correspondente ao seu ID recebe o valor 1, enquanto todas as outras posições são preenchidas com 0. Por exemplo, em um vocabulário de 1000 palavras, a palavra \"gato\" poderia ser representada por um vetor de 1000 posições, com um único 1 na posição do seu ID e zeros em todas as outras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: O rato roeu a roupa do Rei de Roma\n",
      "Tokens IDs: [1, 2, 4, 6, 5, 8, 12, 10, 11]\n",
      "Tokens one-hot: [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "vocab_onehot = {\n",
    "    \"O\": 1,\n",
    "    \"rato\": 2,\n",
    "    \"comeu\": 3,\n",
    "    \"roeu\": 4,\n",
    "    \"roupa\": 5,\n",
    "    \"a\": 6,\n",
    "    \"rainha\": 7,\n",
    "    \"do\": 8,\n",
    "    \"roma\": 9,\n",
    "    \"de\": 10,\n",
    "    \"Roma\": 11,\n",
    "    \"Rei\": 12\n",
    "}\n",
    "\n",
    "vector_onehot = {\n",
    "    k: [1 if i == v else 0 for i in range(1, len(vocab_onehot))] for k, v in vocab_onehot.items()\n",
    "}\n",
    "\n",
    "phrase = \"O rato roeu a roupa do Rei de Roma\"\n",
    "tokens = phrase.split(\" \")\n",
    "\n",
    "print(\"Frase:\", phrase)\n",
    "print(\"Tokens IDs:\", [vocab_onehot[t] for t in tokens])\n",
    "print(\"Tokens one-hot:\", [vector_onehot[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'rato': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'comeu': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'roeu': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'roupa': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " 'a': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " 'rainha': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " 'do': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " 'roma': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " 'de': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " 'Roma': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " 'Rei': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embora o one-hot encoding represente um avanço em relação ao uso direto de IDs numéricos, pois permite uma representação mais estruturada das palavras, esta técnica ainda apresenta limitações significativas que a tornam inadequada para processamento de linguagem natural moderno:\n",
    "\n",
    "1. **Alta Dimensionalidade**: Em vocabulários extensos, que podem conter dezenas ou centenas de milhares de palavras, os vetores tornam-se extremamente longos e esparsos, resultando em uso ineficiente de memória e poder computacional.\n",
    "2. **Falta de Relacionamento Semântico**: Assim como os IDs numéricos simples, o one-hot encoding não captura relações semânticas entre palavras. Por exemplo, \"gato\" e \"felino\" são representados por vetores completamente diferentes, sem qualquer indicação de sua proximidade conceitual.\n",
    "3. **Vetores Esparsos**: Com apenas um valor não-zero em cada vetor, as operações matemáticas tornam-se computacionalmente custosas e ineficientes, especialmente em redes neurais modernas que precisam processar grandes volumes de texto.\n",
    "\n",
    "Outras abordagens tradicionais de vetorização, como Bag of Words (BoW) e Term Frequency-Inverse Document Frequency (TF-IDF), tentam resolver algumas dessas questões ao criar representações para frases e documentos inteiros. Contudo, estas técnicas ainda sofrem com problemas de esparsidade, ausência de relações semânticas e, crucialmente, não preservam a ordem das palavras no texto.\n",
    "\n",
    "Felizmente, essas limitações fundamentais são superadas, ou pelo menos significativamente reduzidas, através do uso de Embeddings\n",
    "\n",
    "## **Embeddings**\n",
    "\n",
    "Para superar essas limitações, surgiram os embeddings — vetores densos e multidimensionais que capturam propriedades semânticas e contextuais das palavras.\n",
    "\n",
    "Para entender melhor, considere uma lista de animais: \"gato\", \"cachorro\", \"cobra\", \"águia\" e \"elefante\". Cada palavra pode ser descrita por características numéricas:\n",
    "\n",
    "![Representação Vetorial](../assets/3-REPRESENTACAO_VETORIAL.png)\n",
    "\n",
    "Podemos representar **Aranha** com o vetor [8, 0, 0.02] e **Cachorro** com o vetor [4, 1, 7]. Esta representação permite que o modelo identifique similaridades entre \"gato\" e \"cachorro\" através de suas características compartilhadas, ao mesmo tempo que reconhece as diferenças contextuais entre eles e \"cobra\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "color=Gato<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>text=%{text}<extra></extra>",
         "legendgroup": "Gato",
         "marker": {
          "color": "#636efa",
          "size": 8,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "Gato",
         "scene": "scene",
         "showlegend": true,
         "text": [
          "Gato"
         ],
         "textfont": {
          "size": 12
         },
         "textposition": "top left",
         "type": "scatter3d",
         "x": [
          4
         ],
         "y": [
          1
         ],
         "z": [
          4
         ]
        },
        {
         "hovertemplate": "color=Cachorro<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>text=%{text}<extra></extra>",
         "legendgroup": "Cachorro",
         "marker": {
          "color": "#EF553B",
          "size": 8,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "Cachorro",
         "scene": "scene",
         "showlegend": true,
         "text": [
          "Cachorro"
         ],
         "textfont": {
          "size": 12
         },
         "textposition": "top left",
         "type": "scatter3d",
         "x": [
          4
         ],
         "y": [
          1
         ],
         "z": [
          7
         ]
        },
        {
         "hovertemplate": "color=Águia<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>text=%{text}<extra></extra>",
         "legendgroup": "Águia",
         "marker": {
          "color": "#00cc96",
          "size": 8,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "Águia",
         "scene": "scene",
         "showlegend": true,
         "text": [
          "Águia"
         ],
         "textfont": {
          "size": 12
         },
         "textposition": "top left",
         "type": "scatter3d",
         "x": [
          2
         ],
         "y": [
          0
         ],
         "z": [
          5
         ]
        },
        {
         "hovertemplate": "color=Aranha<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>text=%{text}<extra></extra>",
         "legendgroup": "Aranha",
         "marker": {
          "color": "#ab63fa",
          "size": 8,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "Aranha",
         "scene": "scene",
         "showlegend": true,
         "text": [
          "Aranha"
         ],
         "textfont": {
          "size": 12
         },
         "textposition": "top left",
         "type": "scatter3d",
         "x": [
          8
         ],
         "y": [
          0
         ],
         "z": [
          0.02
         ]
        },
        {
         "hovertemplate": "color=Papagaio<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>text=%{text}<extra></extra>",
         "legendgroup": "Papagaio",
         "marker": {
          "color": "#FFA15A",
          "size": 8,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "Papagaio",
         "scene": "scene",
         "showlegend": true,
         "text": [
          "Papagaio"
         ],
         "textfont": {
          "size": 12
         },
         "textposition": "top left",
         "type": "scatter3d",
         "x": [
          2
         ],
         "y": [
          0
         ],
         "z": [
          1
         ]
        },
        {
         "hovertemplate": "color=Cobra<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>text=%{text}<extra></extra>",
         "legendgroup": "Cobra",
         "marker": {
          "color": "#19d3f3",
          "size": 8,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "Cobra",
         "scene": "scene",
         "showlegend": true,
         "text": [
          "Cobra"
         ],
         "textfont": {
          "size": 12
         },
         "textposition": "top left",
         "type": "scatter3d",
         "x": [
          0
         ],
         "y": [
          0
         ],
         "z": [
          15
         ]
        }
       ],
       "layout": {
        "height": 500,
        "legend": {
         "title": {
          "text": "color"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "b": 50,
         "l": 20,
         "r": 20,
         "t": 50
        },
        "scene": {
         "aspectratio": {
          "x": 1.2,
          "y": 1,
          "z": 0.7
         },
         "camera": {
          "eye": {
           "x": 1.5,
           "y": 1.1,
           "z": 1.5
          }
         },
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "Patas"
          }
         },
         "yaxis": {
          "title": {
           "text": "Mamífero"
          }
         },
         "zaxis": {
          "title": {
           "text": "Peso"
          }
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Embedding de Animais"
        },
        "width": 600
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dicionário com palavras e vetores de 3 posições\n",
    "dict_animals = {\n",
    "    \"Gato\": [4, 1, 4],          # 4 patas, mamífero, 4 kg\n",
    "    \"Cachorro\": [4, 1, 7],      # 4 patas, mamífero, 7 kg\n",
    "    \"Águia\": [2, 0, 5],         # 2 patas, não mamífero, 5 kg\n",
    "    \"Aranha\": [8, 0, 0.02],     # 8 patas, não mamífero, 0.02 kg\n",
    "    \"Papagaio\": [2, 0, 1],      # 2 patas, não mamífero, 1 kg\n",
    "    \"Cobra\": [0, 0, 15],        # 0 patas, não mamífero, 15 kg\n",
    "}\n",
    "\n",
    "words = list(dict_animals.keys())\n",
    "vectors = list(dict_animals.values())\n",
    "x = [vec[0] for vec in vectors]\n",
    "y = [vec[1] for vec in vectors]\n",
    "z = [vec[2] for vec in vectors]\n",
    "\n",
    "fig = px.scatter_3d(x=x, y=y, z=z, color=words, text=words)\n",
    "\n",
    "fig.update_traces(\n",
    "    marker=dict(size=8), \n",
    "    textposition='top left',\n",
    "    textfont_size=12,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Embedding de Animais',\n",
    "    scene = dict(\n",
    "        xaxis_title='Patas',\n",
    "        yaxis_title='Mamífero',\n",
    "        zaxis_title='Peso'\n",
    "    ),\n",
    "    scene_aspectratio=dict(x=1.2, y=1, z=.7),\n",
    "    scene_camera_eye=dict(x=1.5, y=1.1, z=1.5),\n",
    "    margin=dict(l=20, r=20, b=50, t=50),\n",
    "    showlegend=False,\n",
    "    width=600,\n",
    "    height=500,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao plotarmos esses animais em um espaço onde cada eixo representa uma propriedade, notamos que animais similares, como águia e papagaio ou cachorro e gato, ficam próximos uns aos outros, enquanto aranha e cobra se posicionam distantes.\n",
    "\n",
    "Agora imagine que, em vez de animais, temos palavras (ou tokens) e, em vez de 3 características, temos centenas ou milhares. Se pudéssemos visualizar esse espaço (conhecido como espaço latente ou espaço de embedding), veríamos que palavras semelhantes como \"rei\" e \"rainha\" ficariam próximas, assim como \"Brasília\" e \"Brasil\". Notaríamos também que \"rei\" e \"rainha\" mantêm a mesma relação que \"homem\" e \"mulher\", preservando distâncias equivalentes.\n",
    "\n",
    "Os embeddings são, essencialmente, representações vetoriais de elementos não numéricos (como palavras ou frases) em espaços multidimensionais. Em vez de definirmos manualmente as features, como fizemos com os animais (número de patas, peso, etc.), os valores e suas propriedades são aprendidos automaticamente durante o treinamento.\n",
    "\n",
    "Para criarmos embeddings no PyTorch, basta usarmos o nn.Embedding, informando o tamanho do vocabulário e a dimensão do embedding (número de features). A camada de embedding funciona como uma grande tabela que relaciona cada ID ao seu vetor correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5000, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Embedding(\n",
    "  num_embeddings=5000,    # Tamanho do Vocabulário \n",
    "  embedding_dim=300       # Tamanho do Embedding (número de features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos criar uma camada de Embedding e aplicá-la em um exemplo prático. A partir deste ponto, passaremos a chamar os vetores de tensores. Embora o conceito fundamental seja o mesmo, no contexto de Deep Learning é convenção nos referirmos a vetores e matrizes como tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: O rato roeu a roupa do Rei de Roma\n",
      "Tokens IDs: [1, 2, 4, 6, 5, 8, 12, 10, 11]\n",
      "Tokens embeddings: tensor([[-3.9248e-01, -1.4036e+00, -7.2788e-01, -5.5943e-01, -7.6884e-01,\n",
      "          7.6245e-01,  1.6423e+00, -1.5960e-01, -4.9740e-01,  4.3959e-01],\n",
      "        [-7.5813e-01,  1.0783e+00,  8.0080e-01,  1.6806e+00,  1.2791e+00,\n",
      "          1.2964e+00,  6.1047e-01,  1.3347e+00, -2.3162e-01,  4.1759e-02],\n",
      "        [-1.5576e+00,  9.9564e-01, -8.7979e-01, -6.0114e-01, -1.2742e+00,\n",
      "          2.1228e+00, -1.2347e+00, -4.8791e-01, -9.1382e-01, -6.5814e-01],\n",
      "        [-9.7807e-02,  1.8446e+00, -1.1845e+00,  1.3835e+00,  1.4451e+00,\n",
      "          8.5641e-01,  2.2181e+00,  5.2317e-01,  3.4665e-01, -1.9733e-01],\n",
      "        [ 7.8024e-02,  5.2581e-01, -4.8799e-01,  1.1914e+00, -8.1401e-01,\n",
      "         -7.3599e-01, -1.4032e+00,  3.6004e-02, -6.3477e-02,  6.7561e-01],\n",
      "        [ 1.0868e-02, -3.3874e-01, -1.3407e+00, -5.8537e-01,  5.3619e-01,\n",
      "          5.2462e-01,  1.1412e+00,  5.1644e-02,  7.4395e-01, -4.8158e-01],\n",
      "        [-3.6091e-01, -6.0590e-02, -1.8058e+00,  9.2543e-01, -3.7534e-01,\n",
      "          1.0331e+00, -6.8665e-01,  6.3681e-01,  2.1755e-01, -4.6655e-02],\n",
      "        [ 6.4076e-01,  5.8325e-01,  1.0669e+00, -4.5015e-01, -1.8527e-01,\n",
      "          7.5276e-01,  4.0476e-01,  1.7847e-01,  2.6491e-01,  1.2732e+00],\n",
      "        [-1.3109e-03, -3.0360e-01, -1.4570e+00, -1.0234e-01, -1.4364e+00,\n",
      "         -1.1299e+00, -1.3603e-01,  1.6354e+00,  6.5474e-01,  5.7600e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "VOCAB = {\n",
    "    \"O\": 1,\n",
    "    \"rato\": 2,\n",
    "    \"comeu\": 3,\n",
    "    \"roeu\": 4,\n",
    "    \"roupa\": 5,\n",
    "    \"a\": 6,\n",
    "    \"rainha\": 7,\n",
    "    \"do\": 8,\n",
    "    \"roma\": 9,\n",
    "    \"Roma\": 11,\n",
    "    \"de\": 10,\n",
    "    \"Rei\": 12\n",
    "}\n",
    "\n",
    "VOCAB_SIZE = len(VOCAB) # Tamanho do nosso vocabulário\n",
    "EMBEDDING_DIM = 10 # Dimensão dos vetores de embedding\n",
    "\n",
    "torch.manual_seed(42)\n",
    "embedding_layer = nn.Embedding(num_embeddings=VOCAB_SIZE+1, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "phrase = \"O rato roeu a roupa do Rei de Roma\"\n",
    "tokens = phrase.split(\" \")\n",
    "\n",
    "input_ids = torch.tensor([VOCAB[t] for t in tokens])  # IDs dos tokens\n",
    "vectors = embedding_layer(input_ids)\n",
    "\n",
    "print(\"Frase:\", phrase)\n",
    "print(\"Tokens IDs:\", [VOCAB[t] for t in tokens])\n",
    "print(\"Tokens embeddings:\", vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de uma única frase, realizamos um processo básico de tokenização, convertemos os tokens em IDs usando um vocabulário e obtivemos seus respectivos tensores na camada de Embedding.\n",
    "\n",
    "É importante destacar que, neste estágio, esses tensores ainda **não têm significado** — são apenas números aleatórios gerados, pois a camada de Embedding ainda não foi treinada. Assim, os valores que deveriam representar o significado de cada token ainda não foram aprendidos.\n",
    "\n",
    "### **Dimensionalidade e Eficiência**\n",
    "\n",
    "A escolha da dimensionalidade dos embeddings é crucial. Dimensões menores processam mais rapidamente, mas podem não capturar nuances suficientes. Já dimensões muito grandes aumentam o custo computacional e o consumo de memória.\n",
    "\n",
    "> O modelo GPT-3 da OpenAI utiliza embeddings com 12.288 dimensões.\n",
    "> \n",
    "\n",
    "O tamanho do vocabulário é fundamental. Vocabulários grandes demandam mais memória para processar embeddings, mas permitem capturar maior diversidade linguística.\n",
    "\n",
    "A dimensionalidade também afeta diretamente a capacidade do modelo de capturar relações complexas. O tamanho da camada de embedding é uma decisão crítica que influencia tanto o desempenho quanto o custo computacional do modelo.\n",
    "\n",
    "## **Codificação Posicional: Incorporando Ordem**\n",
    "\n",
    "Modelos baseados em RNNs possuem um viés indutivo natural — suas características estruturais já incluem pressupostos sobre a organização dos dados. Como os tokens são processados sequencialmente, a posição das palavras na frase é naturalmente incorporada ao fluxo de cálculos, eliminando a necessidade de informar explicitamente a ordem temporal do texto.\n",
    "\n",
    "Já os Transformers não possuem esse viés indutivo intrínseco. Por processarem todos os tokens simultaneamente, sua arquitetura não captura naturalmente a posição das palavras no texto. Isso representa um desafio significativo, pois a ordem das palavras pode alterar completamente o significado de uma frase — \"o cachorro mordeu o homem\" tem um significado muito diferente de \"o homem mordeu o cachorro\".\n",
    "\n",
    "Para que os Transformers compreendam a estrutura do texto, precisamos adicionar informações explícitas sobre a posição de cada token. Isso é feito através do positional encoding, que cria nossos embeddings posicionais.\n",
    "\n",
    "Esses embeddings posicionais funcionam de maneira análoga aos embeddings de tokens, mas em vez de codificar o significado da palavra, codificam sua posição na sequência. O processo final envolve somar os dois embeddings: o do token (que carrega o significado) e o posicional (que indica a posição). Esta abordagem permite que os Transformers processem as entradas em paralelo enquanto preservam a compreensão da estrutura sequencial do texto.\n",
    "\n",
    "Existem diferentes abordagens para implementar o positional encoding. A técnica original, apresentada no artigo \"Attention is All You Need\", utiliza funções trigonométricas (seno e cosseno) para gerar os embeddings posicionais de forma determinística. Esta solução tem a vantagem de não adicionar parâmetros treináveis ao modelo.\n",
    "\n",
    "Uma alternativa é utilizar uma camada adicional de embedding que aprende as representações posicionais durante o treinamento. Embora esta abordagem adicione parâmetros ao modelo, ela foi adotada com sucesso em arquiteturas posteriores como BERT e GPT-2, demonstrando resultados equivalentes à solução trigonométrica.\n",
    "\n",
    "![positional_encoding](../assets/3-EMBEDDING%20POSICIONAL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nosso modelo, usaremos os embeddings posicionais treináveis - uma camada adicional que aprende a representar a posição de cada token (0, 1, 2...) durante o treinamento. Isso difere da camada principal de embeddings, que trabalha com os IDs das palavras do vocabulário.\n",
    "\n",
    "## **Implementando nossa solução**\n",
    "\n",
    "Para criarmos a camada de embedding posicional do nosso modelo, podemos utilizar a função disponível na biblioteca Pytorch. Diferentemente da criação de embeddings convencionais, em vez de especificarmos o tamanho do vocabulário, devemos informar a quantidade máxima de tokens que o modelo será capaz de processar, conhecida como **janela de contexto.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: O rato roeu a roupa do Rei de Roma\n",
      "--- Embedding Token 1:\n",
      " tensor([-0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624,  1.6423, -0.1596,\n",
      "        -0.4974,  0.4396], grad_fn=<SelectBackward0>)\n",
      "Shape: torch.Size([9, 10])\n",
      "--- Embedding Posicional 1:\n",
      " tensor([ 1.3541, -0.0709,  2.1726,  2.2334,  0.2225, -0.9300,  0.3208,  0.0371,\n",
      "         0.5675,  0.8284], grad_fn=<SelectBackward0>)\n",
      "Shape: torch.Size([9, 10])\n",
      "--- Embedding Final 1:\n",
      " tensor([ 0.9617, -1.4745,  1.4447,  1.6740, -0.5463, -0.1676,  1.9632, -0.1225,\n",
      "         0.0701,  1.2680], grad_fn=<SelectBackward0>)\n",
      "Shape: torch.Size([9, 10])\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_LENGTH =  20# Número máximo de tokens que o modelo pode considerar (Janela de contexto)\n",
    "\n",
    "# Criando a camada de embedding para posições\n",
    "pos_embedding_layer = torch.nn.Embedding(CONTEXT_LENGTH, EMBEDDING_DIM)\n",
    "\n",
    "# Recuperando os embeddings posicionais para cada token\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(len(tokens)))\n",
    "\n",
    "# Somando os embeddings token e posicional para obter o embedding final\n",
    "embedding = vectors + pos_embeddings\n",
    "\n",
    "print(\"Frase:\", phrase)\n",
    "print(\"--- Embedding Token 1:\\n\", vectors[0])\n",
    "print(\"Shape:\", vectors.size())\n",
    "print(\"--- Embedding Posicional 1:\\n\", pos_embeddings[0])\n",
    "print(\"Shape:\", pos_embeddings.size())\n",
    "print(\"--- Embedding Final 1:\\n\", embedding[0])\n",
    "print(\"Shape:\", embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código demonstrado, observamos como os embeddings finais são criados através de um processo de soma entre dois componentes: os embeddings dos tokens e os embeddings posicionais. Esta operação matemática permite que cada token mantenha sua representação semântica original enquanto incorpora informações sobre sua localização na sequência.\n",
    "\n",
    "Durante este processo, o modelo mantém a representação original de cada token através do embedding de token, enquanto adiciona uma camada extra de informação posicional através do embedding posicional. Esta combinação resulta em um tensor que carrega tanto o significado quanto a posição da palavra no texto.\n",
    "\n",
    "Esta abordagem permite que o modelo processe simultaneamente a semântica e a estrutura da frase, fornecendo uma base sólida para o aprendizado do nosso LLM.\n",
    "\n",
    "## **Conclusão**\n",
    "\n",
    "Os embeddings representam um pilar fundamental no processamento de linguagem natural, transformando texto em representações matemáticas precisas que permitem aos modelos de aprendizado de máquina compreender e processar a linguagem humana. Através dessas estruturas matemáticas, os modelos desenvolvem uma compreensão sofisticada de nuances linguísticas e relações contextuais.\n",
    "\n",
    "A integração de embeddings com mecanismos avançados, especialmente os posicionais, impulsiona o desempenho excepcional dos modelos de linguagem modernos. Esta combinação permite que eles compreendam não apenas o significado das palavras, mas também sua posição e relacionamento dentro do texto.\n",
    "\n",
    "Em nossa próxima discussão, mergulharemos nos **mecanismos de atenção** - um componente revolucionário dos Transformers que trabalha em harmonia com os embeddings. Exploraremos como esses mecanismos permitem que os modelos focalizem dinamicamente nas partes mais relevantes do texto, possibilitando uma compreensão profunda e uma geração de texto mais coerente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
