{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Construindo um LLM do Zero: Treinando um Foundational Model\n",
    "\n",
    "Este é o **sétimo** de uma série de oito artigos que podem ser encontrados no meu medium. Acesse o primeiro artigo da série aqui: [Construindo um LLM: entendendo os Grandes Modelos de Linguagem](https://blog.zfab.me/construindo-um-llm-entendendo-os-grandes-modelos-de-linguagem-b37884219eaa)\n",
    "\n",
    "----\n",
    "Com a arquitetura do nosso modelo concluída, alcançamos o primeiro objetivo desta série: construir um LLM do zero. No entanto, nosso modelo atual consiste apenas em matrizes aleatórias que produzem texto sem significado. Neste artigo, daremos um passo crucial: transformar esse modelo em uma ferramenta capaz de gerar texto (um pouco mais) coerente.\n",
    "\n",
    "Para alcançar esse objetivo, precisamos treinar o modelo com um imenso volume de texto, ensinando-o a prever o próximo token com base nos anteriores. Este processo, conhecido como pré-treino, é o que nos entregará nosso Modelo Fundacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\"font.size\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, d_in, d_out, context_length, n_heads, qkv_bias=False, dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_out % n_heads == 0, \"d_out deve ser divisível por num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_out // n_heads\n",
    "\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        self.out_proj = torch.nn.Linear(d_out, d_out)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_batch, n_tokens, _ = x.size()\n",
    "\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        queries = queries.view(n_batch, n_tokens, self.n_heads, self.d_head)\n",
    "        keys = keys.view(n_batch, n_tokens, self.n_heads, self.d_head)\n",
    "        values = values.view(n_batch, n_tokens, self.n_heads, self.d_head)\n",
    "\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(-2, -1)\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:n_tokens, :n_tokens], -torch.inf\n",
    "        )\n",
    "\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.size(-1) ** 0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vector = (attention_weights @ values).transpose(1, 2)\n",
    "        context_vector = context_vector.contiguous().view(n_batch, n_tokens, self.d_out)\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_emb):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-6  # Epsilon para evitar divisão por zero\n",
    "        self.scale = nn.Parameter(\n",
    "            torch.ones(d_emb)\n",
    "        )  # Gamma - Inicializando com 1 para não alterar a escala\n",
    "        self.shift = nn.Parameter(\n",
    "            torch.zeros(d_emb)\n",
    "        )  # Beta - Inicializando com 0 para não alterar o deslocamento\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift  # Gamma * x_norm + Beta\n",
    "\n",
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1\n",
    "                + torch.tanh(\n",
    "                    torch.sqrt(torch.tensor(2.0 / torch.pi))\n",
    "                    * (x + 0.044715 * torch.pow(x, 3))\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_emb):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(d_emb, d_emb * 4),\n",
    "            GeLU(),\n",
    "            nn.Linear(d_emb * 4, d_emb),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_emb, n_heads, context_length, dropout=0.1, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(\n",
    "            d_in=d_emb,\n",
    "            d_out=d_emb,\n",
    "            context_length=context_length,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            qkv_bias=qkv_bias,\n",
    "        )\n",
    "        self.ff = FeedForward(d_emb)\n",
    "        self.norm1 = LayerNorm(d_emb)\n",
    "        self.norm2 = LayerNorm(d_emb)\n",
    "        self.drop_shortcut = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)  # 1\n",
    "        x = self.mha(x)  # 2\n",
    "        x = self.drop_shortcut(x)  # 3\n",
    "        x = x + shortcut  # 4\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)  # 5\n",
    "        x = self.ff(x)  # 6\n",
    "        x = self.drop_shortcut(x)  # 7\n",
    "        x = x + shortcut  # 8\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_vocab, d_emb, context_length, n_layers, n_heads, dropout, qkv_bias\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "\n",
    "        # Embedding de tokens e de posição\n",
    "        self.tok_emb = nn.Embedding(d_vocab, d_emb)\n",
    "        self.pos_emb = nn.Embedding(context_length, d_emb)\n",
    "        self.drop_emb = nn.Dropout(dropout)\n",
    "\n",
    "        # Sequência de Blocos Transformers\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(d_emb, n_heads, context_length, dropout, qkv_bias)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Normalização e projeção para o vocabulário\n",
    "        self.final_norm = LayerNorm(d_emb)\n",
    "        self.out_head = nn.Linear(d_emb, d_vocab, bias=False)\n",
    "\n",
    "        # Inicialização dos pesos\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # Weight tying\n",
    "        self.out_head.weight = self.tok_emb.weight\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        # Embedding de tokens e de posição\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # Sequência de Blocos Transformers\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        # Normalização e projeção para o vocabulário\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "    # Função para gerar texto\n",
    "    def generate(self, input, max=200, temperature=1.0, top_k=0, top_p=1.0):\n",
    "        for _ in tqdm(range(max), desc=\"Gerando Tokens...\"):\n",
    "            input = input[\n",
    "                :, -self.context_length :\n",
    "            ]  # Garantindo que o seja no máximo do tamanho do contexto.\n",
    "            logits = self(input)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Aplicando temperature\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Aplicando top_k se especificado\n",
    "            if top_k > 0:\n",
    "                top_k_values, top_k_indices = torch.topk(\n",
    "                    logits, min(top_k, logits.size(-1))\n",
    "                )\n",
    "                logits = torch.full_like(logits, float(\"-inf\"))\n",
    "                logits.scatter_(1, top_k_indices, top_k_values)\n",
    "\n",
    "            # Aplicando top_p (nucleus sampling) se menor que 1.0\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(\n",
    "                    F.softmax(sorted_logits, dim=-1), dim=-1\n",
    "                )\n",
    "\n",
    "                # Remove tokens com probabilidade cumulativa acima do threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                # Shift para manter pelo menos um token\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[..., indices_to_remove] = float(\"-inf\")\n",
    "\n",
    "            # Convertendo para probabilidades e amostrando\n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(prob, num_samples=1)\n",
    "            input = torch.cat([input, next_token], dim=-1)\n",
    "\n",
    "            if next_token == 3:  # Token de fim de texto\n",
    "                break\n",
    "\n",
    "        return input\n",
    "    \n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_sample(model, device, tokenizer, input_text, max_token=200, temperature=1, top_k=0, top_p=1):\n",
    "    input_tokens = torch.tensor(tokenizer.encode(input_text), dtype=torch.long)\n",
    "    input_tokens = input_tokens.unsqueeze(0).to(device)\n",
    "\n",
    "    output = model.generate(input_tokens, max_token, temperature, top_k, top_p)\n",
    "    output_text = tokenizer.decode(output.squeeze().tolist())\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Fundacionais\n",
    "\n",
    "Um Modelo Fundacional (Foundational Model) é um LLM pré-treinado em uma vasta quantidade de dados não rotulados — no nosso caso, textos de diversas origens. São chamados “fundacionais” porque estabelecem uma base de conhecimento que pode ser adaptada para múltiplos propósitos.\n",
    "\n",
    "Os modelos opensource, como os que encontramos no HuggingFace, eles são identificados com sufixos como “-base” ou “-pt” (pré-treinado). Por exemplo, “bert-base” ou “gemma3–4b-pt” indicam versões fundacionais que passaram apenas pela fase de pré-treino, sem especialização para tarefas específicas.\n",
    "\n",
    "Por si só, esses modelos pré-treinados são excelentes completadores de frases: dados os tokens anteriores, eles preveem o próximo e, ao realizar esse processo recursivamente, geram conteúdo.\n",
    "\n",
    "Uma descoberta interessante foi a capacidade desses modelos de realizar tarefas sem treino específico, conhecido como “**zero-shot learning**”. Pesquisadores observaram que modelos pré-treinados em grandes volumes de texto conseguiam responder perguntas, classificar textos e até resolver problemas simples de raciocínio sem nunca terem sido explicitamente treinados para isso.\n",
    "\n",
    "Esse fenômeno sugere que o processo de pré-treino não apenas ensina o modelo a prever palavras, mas também permite que ele desenvolva uma compreensão mais profunda da linguagem e do conhecimento contido nos textos. No entanto, o desempenho zero-shot geralmente é inferior ao de modelos especializados, justificando a necessidade de fine-tuning para tarefas específicas como conversação ou classificação de texto.\n",
    "\n",
    "# Processo de Treinamento\n",
    "\n",
    "Vamos começar instanciando nosso modelo e gerando alguns textos para avaliarmos após o pré treinamento seu desempenho.\n",
    "\n",
    "Vamos instanciar nosso tokenizer, que criamos no segundo artigo da série, e o `GPTModel` que finalizamos no artigo passado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de parâmetros: 88,398,336\n"
     ]
    }
   ],
   "source": [
    "# Carregando o Tokenizer e o Modelo\n",
    "tk = spm.SentencePieceProcessor(\n",
    "    model_file=\"../models/gigaverbo_tk.model\"\n",
    ")\n",
    "\n",
    "MODEL = GPTModel(\n",
    "    d_vocab=tk.vocab_size(),\n",
    "    d_emb=768,\n",
    "    context_length=256,\n",
    "    n_layers=12,\n",
    "    n_heads=12,\n",
    "    dropout=0.1,\n",
    "    qkv_bias=True,\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in MODEL.parameters())\n",
    "print(f\"Total de parâmetros: {total_params:,}\")\n",
    "\n",
    "# OUTPUT\n",
    "# Total de parâmetros: 88,398,336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso objetivo não é criar uma nova família de modelos ou competir com as Big Techs, por isso nosso modelo será pequeno — muito pequeno: apenas **88M de parâmetros**. Para colocar em perspectiva, hoje em dia até os modelos considerados pequenos têm alguns **bilhões de parâmetros**.\n",
    "\n",
    "Como nosso foco é entender o funcionamento dos LLMs, desde sua arquitetura até seu treinamento, não precisamos gastar processamento com um modelo gigante. O que diferencia nosso modelo dos demais será basicamente seu tamanho, a quantidade de tokens processados no pré-treinamento e, consequentemente, o tempo de treinamento.\n",
    "\n",
    "Se você tiver uma GPU disponível e quiser treinar um modelo maior, poderá usar o mesmo código — basta ajustar parâmetros como janela de contexto, número de blocos transformers e tamanho dos embeddings.\n",
    "\n",
    "Por enquanto, para nosso objetivo de aprendizado, esse modelinho será mais que suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 30/30 [00:02<00:00, 14.02it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:02<00:00, 11.37it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:02<00:00, 12.81it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:02<00:00, 14.39it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:02<00:00, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta 1: O Brasil é umcho ó estiver veíançasTu Geren clima Mo ó estabel Colhoumou� poderiacu confi\u0018ais porporte entrada tur[stitumanha ent\n",
      "Resposta 2: O Brasil é umussãochouas chamadaram capvenção atual dicas veí existemst frequentementehou venchocho veí veíanotação diferentehou Apple Alguns jorn prov sites combussão\n",
      "Resposta 3: O Brasil é um prov dim cap metas pol Com paiselo af comb maiorementwnica sono fazendoohpidamenteíqu essencial Apple identicaicapres dada existem\u0015 sono grav\n",
      "Resposta 4: O Brasil é um poleloenef Austeloícileleza Ol financeiros Apple conexug Austcho mercado dor filmebora alternastitusarb met ing ele alternatur[ Califórnia us\n",
      "Resposta 5: O Brasil é umCrie produzeral po\u0015 víde M empresastur Eu Apple ingonesongão estiver especial melhor mem�� natural empresaicamente específica resul\u0003 ent gencho\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i in range(5):\n",
    "    response = generate_sample(\n",
    "        model=MODEL, \n",
    "        device=\"cpu\", \n",
    "        tokenizer=tk, \n",
    "        input_text=\"O Brasil é um\", \n",
    "        max_token=30, \n",
    "        temperature=0.4,\n",
    "        top_k=40,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    responses.append(response)\n",
    "print(\"\")\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Resposta {i+1}: {response}\")\n",
    "\n",
    "#OUTPUT\n",
    "# Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 15.56it/s]\n",
    "# Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 17.86it/s]\n",
    "# Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 18.20it/s]\n",
    "# Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 17.56it/s]\n",
    "# Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 21.82it/s]\n",
    "#\n",
    "# Resposta 1: O Brasil é umiros alterna frase Terposição maneirasró ide ganiros� benefíciosbilidades bas baix tele baixaérciobilidades necessidade acab específicasab espera negócios� fecoocê\n",
    "# Resposta 2: O Brasil é um descont Eleeio fe relexto programater.bilidades legumes resumociar sequin bollialia noviros baixa eventos muitoodos tradicionalhhe sequ tentume\n",
    "# Resposta 3: O Brasil é umbilidades minha mund x fraseá mó medoientocê minhatório programa situocê mon Pateretes vi espa profissionalard\u0004 Googleale notícias nov baixaocê\n",
    "# Resposta 4: O Brasil é ummicas interaçãoando especial\" candida Reitos prática Sim passos na tarocêritân emissões partetional amigo Falvamalidade tradicional parte benefícios usar iP muito facil facil\n",
    "# Resposta 5: O Brasil é um candidats saudável perguntasYáticas negócios frutas tele Ol descont capacidadearami cai� tele emp usadasumeing Ro lamico escolhabilidades negóciosbilidades segundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, nosso modelo atual é apenas um gerador de palavras sem sentido. Ao fornecer a entrada \"O Brasil é um\", ele gera frases completamente aleatórias. Nosso objetivo é que, após o pré-treinamento, o modelo seja capaz de construir frases mais coerentes.\n",
    "\n",
    "## Preparando o Dataset\n",
    "\n",
    "A preparação adequada dos dados é um elemento crítico no treinamento de modelos de linguagem. O processo envolve fornecer tokens de entrada ao modelo para que ele aprenda a prever o próximo token da sequência.\n",
    "\n",
    "Para isso, precisamos estruturar nosso dataset de maneira específica, permitindo que o modelo desenvolva a capacidade de fazer previsões contextualizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 1\n",
      "Input:\t['O', 'Brasil', 'é']\n",
      "Output:\t['Brasil', 'é', 'um']\n",
      "---\n",
      "Iteração 2\n",
      "Input:\t['Brasil', 'é', 'um']\n",
      "Output:\t['é', 'um', 'país']\n",
      "---\n",
      "Iteração 3\n",
      "Input:\t['é', 'um', 'país']\n",
      "Output:\t['um', 'país', 'muito']\n",
      "---\n",
      "Iteração 4\n",
      "Input:\t['um', 'país', 'muito']\n",
      "Output:\t['país', 'muito', 'bonito']\n",
      "---\n",
      "Iteração 5\n",
      "Input:\t['país', 'muito', 'bonito']\n",
      "Output:\t['muito', 'bonito', '<|eos|>']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "phrase = \"O Brasil é um país muito bonito <|eos|>\"\n",
    "words = phrase.split()\n",
    "\n",
    "context_window = 3\n",
    "\n",
    "for i in range(len(words)-context_window):\n",
    "    print(\"Iteração\", i+1)\n",
    "    print(f\"Input:\\t{words[i:i+context_window]}\")\n",
    "    print(f\"Output:\\t{words[i+1:i+context_window+1]}\")\n",
    "    print(\"---\")\n",
    "\n",
    "#OUTPUT\n",
    "# Iteração 1\n",
    "# Input:\t['O', 'Brasil', 'é']\n",
    "# Output:\t['Brasil', 'é', 'um']\n",
    "# ---\n",
    "# Iteração 2\n",
    "# Input:\t['Brasil', 'é', 'um']\n",
    "# Output:\t['é', 'um', 'país']\n",
    "# ---\n",
    "# Iteração 3\n",
    "# Input:\t['é', 'um', 'país']\n",
    "# Output:\t['um', 'país', 'muito']\n",
    "# ---\n",
    "# Iteração 4\n",
    "# Input:\t['um', 'país', 'muito']\n",
    "# Output:\t['país', 'muito', 'bonito']\n",
    "# ---\n",
    "# Iteração 5\n",
    "# Input:\t['país', 'muito', 'bonito']\n",
    "# Output:\t['muito', 'bonito', '<|eos|>']\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, usando uma janela de contexto de 3 tokens e a frase `O Brasil é um país muito bonito <|eos|>` como dado de treinamento, precisamos estruturar o dataset para ter 3 tokens de entrada, com a saída sendo os mesmos 3 tokens deslocados uma posição para a direita.\n",
    "\n",
    "Por exemplo, quando usamos como input o início `O Brasil é`, o output correspondente será `Brasil é um` , assim o modelo aprenderá a prever qual token deve vir após certas palavras, dado certos contextos.\n",
    "\n",
    "Na hora de preparar o dataset, além da janela de contexto, para podermos “separar” os inputs, outra informação importante é o que chamamos de stride, que determina o tamanho do \"passo\" entre as janelas de contexto consecutivas.\n",
    "\n",
    "Um stride menor cria mais sobreposição entre as janelas, gerando mais dados de treinamento, enquanto um stride maior reduz a sobreposição e economiza memória. A escolha do stride ideal é um trade-off entre volume de dados e eficiência computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 1\n",
      "Input:\t['O', 'Brasil', 'é']\n",
      "Output:\t['Brasil', 'é', 'um']\n",
      "---\n",
      "Iteração 2\n",
      "Input:\t['é', 'um', 'país']\n",
      "Output:\t['um', 'país', 'muito']\n",
      "---\n",
      "Iteração 3\n",
      "Input:\t['país', 'muito', 'bonito']\n",
      "Output:\t['muito', 'bonito', '<|eos|>']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "phrase = \"O Brasil é um país muito bonito <|eos|>\"\n",
    "words = phrase.split()\n",
    "\n",
    "context_window = 3\n",
    "stride = 2\n",
    "\n",
    "for i in range(0, len(words)-context_window, stride):\n",
    "    print(\"Iteração\", (i//stride)+1)\n",
    "    print(f\"Input:\\t{words[i:i+context_window]}\")\n",
    "    print(f\"Output:\\t{words[i+1:i+context_window+1]}\")\n",
    "    print(\"---\")\n",
    "\n",
    "#OUTPUT\n",
    "# Iteração 1\n",
    "# Input:\t['O', 'Brasil', 'é']\n",
    "# Output:\t['Brasil', 'é', 'um']\n",
    "# ---\n",
    "# Iteração 2\n",
    "# Input:\t['é', 'um', 'país']\n",
    "# Output:\t['um', 'país', 'muito']\n",
    "# ---\n",
    "# Iteração 3\n",
    "# Input:\t['país', 'muito', 'bonito']\n",
    "# Output:\t['muito', 'bonito', '<|eos|>']\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao adicionar o stride ao nosso exemplo, vemos que entre as iterações estamos pulando 2 palavras (em vez de uma). Por exemplo, se na primeira janela pegamos `O Brasil é`, na próxima pegaremos `é um país`, pulando `O` e `Brasil`.\n",
    "\n",
    "Com stride igual a 2, obtivemos apenas 3 conjuntos de inputs com a frase, em comparação aos 5 conjuntos obtidos sem stride (ou com stride igual a 1).\n",
    "\n",
    "Para nosso treinamento, precisaremos de mais conteúdo que apenas uma frase. Por isso, criaremos o dataset com o mesmo corpus usado para gerar o tokenizer: uma parte do dataset **GigaVerbo**. Nosso corpus contém aproximadamente 25 milhões de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tokens no corpus: 24,982,439\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/gigaverbo_tokenized.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_tokenized = f.read()\n",
    "\n",
    "num_tokens = len(text_tokenized.split())\n",
    "print(f\"Número de tokens no corpus: {num_tokens:,}\")\n",
    "\n",
    "#OUTPUT\n",
    "# Número de tokens no corpus: 24,982,439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além de organizar os tokens do nosso corpus nesse formato de input e output, precisamos também agrupar os inputs e seus respectivos outputs esperados (que chamaremos de targets) em conjuntos chamados batches, para otimizar o processo de treinamento.\n",
    "\n",
    "Um batch reúne múltiplos exemplos para processamento simultâneo pelo modelo, tornando o treinamento mais eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (Batch x Trecho x Tokens): (2, 2, 3)\n",
      "\n",
      "Batch 1\n",
      "Input:\n",
      "[['O' 'Brasil' 'é']\n",
      " ['é' 'um' 'país']]\n",
      "Target:\n",
      "[['Brasil' 'é' 'um']\n",
      " ['um' 'país' 'da']]\n",
      "---\n",
      "Batch 2\n",
      "Input:\n",
      "[['país' 'da' 'América']\n",
      " ['América' 'do' 'Sul']]\n",
      "Target:\n",
      "[['da' 'América' 'do']\n",
      " ['do' 'Sul' '<|eos|>']]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "phrase = \"O Brasil é um país da América do Sul <|eos|>\"\n",
    "words = phrase.split()\n",
    "\n",
    "context_window = 3\n",
    "stride = 2\n",
    "batch_size = 2  # Number of examples per batch\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(0, len(words)-context_window, stride):\n",
    "    inputs.append(words[i:i+context_window])\n",
    "    targets.append(words[i+1:i+context_window+1])\n",
    "\n",
    "# Reshape the list to batch_size x num_examples x num_words\n",
    "inputs = np.array(inputs).reshape(batch_size, -1, 3)\n",
    "targets = np.array(targets).reshape(batch_size, -1, 3)\n",
    "\n",
    "print(f\"Shape (Batch x Trecho x Tokens): {inputs.shape}\\n\")\n",
    "for i in range(batch_size):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(f\"Input:\\n{inputs[i]}\")\n",
    "    print(f\"Target:\\n{targets[i]}\")\n",
    "    print(\"---\")\n",
    "\n",
    "#OUTPUT\n",
    "# Shape (Batch x Trecho x Tokens): (2, 2, 3)\n",
    "# \n",
    "# Batch 1\n",
    "# Input:\n",
    "# [['O' 'Brasil' 'é']\n",
    "#  ['é' 'um' 'país']]\n",
    "# Target:\n",
    "# [['Brasil' 'é' 'um']\n",
    "#  ['um' 'país' 'da']]\n",
    "# ---\n",
    "# Batch 2\n",
    "# Input:\n",
    "# [['país' 'da' 'América']\n",
    "#  ['América' 'do' 'Sul']]\n",
    "# Target:\n",
    "# [['da' 'América' 'do']\n",
    "#  ['do' 'Sul' '<|eos|>']]\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer todo esse processamento, usaremos duas classes da biblioteca PyTorch que facilitam a estruturação e o gerenciamento dos inputs e targets usados no treinamento.\n",
    "\n",
    "Criaremos uma classe que herda de Dataset para representar nossos dados, enquanto usaremos o DataLoader para gerenciar o carregamento e a distribuição dos dados durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader(\n",
    "    txt,\n",
    "    batch_size=4,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    dataset = GPTDataset(txt, tk, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparem que nossa classe Dataset está separando o conteúdo passado (`txt`) em inputs e targets usando a mesma lógica que vimos anteriormente. Além disso precisamos implementar a função `__len__` e `__getitem__` que são métodos especiais do Python necessários para que nossa classe funcione corretamente com o DataLoader. O método **len** retorna o tamanho total do dataset, enquanto **getitem** permite acessar um elemento específico através de seu índice.\n",
    "\n",
    "A função que `create_dataloader` que criamos serve para criar o Dataset e passa-lo para um DataLoader que cuidará de gerenciar e separar nossos batchs. Ao criar o Dataloader estamos definindo alguns parametros para configurar como nossos dados serão gerenciados e disponibilizados para o modelo:\n",
    "\n",
    "- `batch_size` : Quantas “frases” teremos dentro de cada batch\n",
    "- `shuffle` : Controla se os dados serão embaralhados aleatoriamente durante o treinamento\n",
    "- `drop_last` : Define se o último batch será descartado caso seja menor que o tamanho definido\n",
    "- `num_workers` : Número de processos paralelos para carregamento dos dados\n",
    "- `pin_memory` : Define se os tensores serão alocados em uma região de memória específica da CPU que permite transferência mais rápida para a GPU\n",
    "\n",
    "Vamos testar e ver se a classe e a função estão funcionando corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Input:\n",
      "tensor([[ 355,  474,  385],\n",
      "        [3268,  320, 3254]])\n",
      "Target:\n",
      "tensor([[ 474,  385,  363],\n",
      "        [ 320, 3254, 4021]])\n",
      "---\n",
      "Batch 2\n",
      "Input:\n",
      "tensor([[1040,  334, 3268],\n",
      "        [ 323,  313, 1040]])\n",
      "Target:\n",
      "tensor([[ 334, 3268,  320],\n",
      "        [ 313, 1040,  334]])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "phrase = \"O Brasil é um país da América do Sul <|eos|>\"\n",
    "dataloader = create_dataloader(phrase, batch_size=2, max_length=3, stride=2)\n",
    "\n",
    "for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    print(f\"Input:\\n{input_ids}\")\n",
    "    print(f\"Target:\\n{target_ids}\")\n",
    "    print(\"---\")\n",
    "\n",
    "#OUTPUT\n",
    "# Batch 1\n",
    "# Input:\n",
    "# tensor([[ 355,  474,  385],\n",
    "#         [ 323,  313, 1040]])\n",
    "# Target:\n",
    "# tensor([[ 474,  385,  363],\n",
    "#         [ 313, 1040,  334]])\n",
    "# ---\n",
    "# Batch 2\n",
    "# Input:\n",
    "# tensor([[1040,  334, 3268],\n",
    "#         [ 385,  363,  323]])\n",
    "# Target:\n",
    "# tensor([[ 334, 3268,  320],\n",
    "#         [ 363,  323,  313]])\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que obtivemos a mesma estrutura do nosso exemplo manual, mas agora com números (os IDs dos tokens) no lugar das palavras.\n",
    "\n",
    "Como em todo processo de treinamento vamos separar uma parte do nosso corpus para validação e manter o restante para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gigaverbo.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(len(text) * train_ratio)\n",
    "\n",
    "train_data = text[:split_idx]\n",
    "val_data = text[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após separar, vamos criar nossos Dataloader, um para treino e outro para validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloader\u001b[49m(\n\u001b[1;32m      2\u001b[0m     train_data,\n\u001b[1;32m      3\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      4\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      5\u001b[0m     stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m      6\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m create_dataloader(\n\u001b[1;32m     12\u001b[0m     val_data,\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_dataloader' is not defined"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m create_dataloader(\n\u001b[1;32m     12\u001b[0m     val_data,\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mcreate_dataloader\u001b[0;34m(txt, batch_size, max_length, stride, shuffle, drop_last, num_workers)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_dataloader\u001b[39m(\n\u001b[1;32m     21\u001b[0m     txt,\n\u001b[1;32m     22\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     28\u001b[0m ):\n\u001b[0;32m---> 29\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mGPTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     31\u001b[0m         dataset,\n\u001b[1;32m     32\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m         pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mGPTDataset.__init__\u001b[0;34m(self, txt, tokenizer, max_length, stride)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token_ids) \u001b[38;5;241m-\u001b[39m max_length, stride):\n\u001b[1;32m      9\u001b[0m     input_chunk \u001b[38;5;241m=\u001b[39m token_ids[i : i \u001b[38;5;241m+\u001b[39m max_length]\n",
      "File \u001b[0;32m~/repos/llm_zero/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py:549\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Encode\u001b[0;34m(self, input, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_EncodeAsImmutableProtoBatch(\u001b[38;5;28minput\u001b[39m, num_threads, enable_sampling, nbest_size,\n\u001b[1;32m    546\u001b[0m                                              alpha, add_bos, add_eos, reverse, emit_unk_piece)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 549\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_EncodeAsIds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_sampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbest_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m                           \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_bos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_eos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memit_unk_piece\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_EncodeAsPieces(\u001b[38;5;28minput\u001b[39m, enable_sampling, nbest_size,\n\u001b[1;32m    553\u001b[0m                               alpha, add_bos, add_eos, reverse, emit_unk_piece)\n",
      "File \u001b[0;32m~/repos/llm_zero/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py:319\u001b[0m, in \u001b[0;36mSentencePieceProcessor._EncodeAsIds\u001b[0;34m(self, text, enable_sampling, nbest_size, alpha, add_bos, add_eos, reverse, emit_unk_piece)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_EncodeAsIds\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, enable_sampling, nbest_size, alpha, add_bos, add_eos, reverse, emit_unk_piece):\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor__EncodeAsIds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_sampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_bos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_eos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memit_unk_piece\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataloader = create_dataloader(\n",
    "    train_data,\n",
    "    batch_size=8,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_dataloader = create_dataloader(\n",
    "    val_data,\n",
    "    batch_size=8,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas configurações e tamanhos de batch são apenas exemplos ilustrativos. As configurações reais utilizadas no treinamento podem ser encontradas no arquivo de treino no github do projeto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memória ocupada pelo dataset de Treino: 678.52 MB\n",
      "Memória ocupada pelo dataset de Validação: 83.88 MB\n"
     ]
    }
   ],
   "source": [
    "def get_tensor_size(tensor):\n",
    "    return tensor.element_size() * tensor.nelement()\n",
    "\n",
    "def get_dataset_tensor_memory(dataset):\n",
    "    input_size = sum(get_tensor_size(tensor) for tensor in dataset.input_ids)\n",
    "    target_size = sum(get_tensor_size(tensor) for tensor in dataset.target_ids)\n",
    "    return input_size + target_size\n",
    "\n",
    "tensor_mem = get_dataset_tensor_memory(train_dataloader.dataset)\n",
    "print(f\"Memória ocupada pelo dataset de Treino: {tensor_mem/1024/1024:.2f} MB\")\n",
    "\n",
    "tensor_mem = get_dataset_tensor_memory(val_dataloader.dataset)\n",
    "print(f\"Memória ocupada pelo dataset de Validação: {tensor_mem/1024/1024:.2f} MB\")\n",
    "\n",
    "#OUTPUT\n",
    "# Memória ocupada pelo dataset de Treino: 678.52 MB\n",
    "# Memória ocupada pelo dataset de Validação: 83.88 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando o consumo de memória inicial, nosso dataset ocupa aproximadamente 760 MB. Quando adicionamos o modelo, que utiliza cerca de 300 MB, chegamos a um total de 1 GB de memória dedicada apenas ao armazenamento dos tensores. Durante o treinamento, esse consumo aumenta significativamente devido à necessidade de armazenar gradientes, estados do otimizador, caches de atenção e outras estruturas temporárias.\n",
    "\n",
    "Isso evidencia que um dos principais desafios no treinamento de modelos de linguagem não se limita apenas ao poder de processamento, mas também à gestão eficiente da memória disponível.\n",
    "\n",
    "Para nosso modelo relativamente pequeno e dataset reduzido, manter todos os dados na memória RAM é viável. No entanto, em cenários de treinamento de modelos maiores com datasets gigantescos, este se torna um dos principais gargalos.\n",
    "\n",
    "Agora que o DataLoader está configurado, podemos passar para o treinamento. Antes, porém, vamos entender como avaliar o desempenho do modelo durante esse processo para ajustar corretamente seus pesos e bias.\n",
    "\n",
    "## A Função de Perda\n",
    "\n",
    "No centro de todo treinamento de modelos está a função de perda (Loss Function), que mede quanto as previsões do modelo se desviam dos valores reais para então ajustar os parâmetros com o objetivo de diminuir progressivamente o erro. No nosso caso, a função de perda indica o quanto o modelo está distante de prever corretamente o próximo token.\n",
    "\n",
    "Existem várias formas de calcular esse valor de perda. Para o tipo de problema que estamos abordando — que é essencialmente um problema de classificação multiclasse — a mais comum é a Cross-Entropy.\n",
    "\n",
    "A Cross-Entropy (Entropia Cruzada) é uma medida da diferença entre duas distribuições de probabilidade: a distribuição prevista pelo modelo e a distribuição real dos dados. Em termos matemáticos, para um problema de classificação com (C) classes, a cross-entropy é definida como:\n",
    "\n",
    "$$\n",
    "H(y, \\hat{y}) = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "•\t$y_i$ é a probabilidade real (geralmente 0 ou 1 em classificação)\n",
    "\n",
    "•\t$\\hat{y}_i$ é a probabilidade prevista pelo modelo\n",
    "\n",
    "Para entendermos como a Cross-Entropy funciona na prática, vamos analisar um exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3])\n",
      "tensor([[ 302, 1680,  306],\n",
      "        [ 306,  364,  269]])\n",
      "---\n",
      "Target shape: torch.Size([2, 3])\n",
      "tensor([[1680,  306,  364],\n",
      "        [ 364,  269, 2169]])\n"
     ]
    }
   ],
   "source": [
    "phrase = \"O gato que gostava de andar de bicicleta\"\n",
    "dataloader = create_dataloader(phrase, batch_size=2, max_length=3, stride=2)\n",
    "\n",
    "first_batch = next(iter(dataloader))\n",
    "\n",
    "print(f\"Input shape: {first_batch[0].shape}\")\n",
    "print(first_batch[0])\n",
    "print(\"---\")\n",
    "print(f\"Target shape: {first_batch[1].shape}\")\n",
    "print(first_batch[1])\n",
    "\n",
    "#OUTPUT\n",
    "# Input shape: torch.Size([2, 3])\n",
    "# tensor([[ 355, 3757,  302],\n",
    "#         [ 306,  364,  269]])\n",
    "# ---\n",
    "# Target shape: torch.Size([2, 3])\n",
    "# tensor([[3757,  302, 1680],\n",
    "#         [ 364,  269, 2169]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, temos duas sequências de entrada e seus respectivos alvos (targets), que são as sequências deslocadas em uma posição. Vamos passar os inputs para o modelo e verificar o que ele está prevendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4096])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = MODEL(first_batch[0])\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "probas.shape\n",
    "\n",
    "#OUTPUT\n",
    "# torch.Size([2, 3, 4096])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos no artigo anterior, nosso modelo retorna um tensor que contém, para cada token em cada frase, a probabilidade de cada um dos 4.096 tokens do vocabulário ser a próxima palavra. Vamos pegar o ID do token com maior probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output previsto (y_hat):\n",
      "[[3857 2090  996]\n",
      " [ 834 2273 1656]]\n"
     ]
    }
   ],
   "source": [
    "tokens_id = torch.argmax(probas, dim=-1)\n",
    "print(f\"Output previsto (y_hat):\\n{tokens_id.numpy()}\")\n",
    "\n",
    "#OUTPUT\n",
    "# Output previsto (y_hat):\n",
    "# [[1452  223 3121]\n",
    "#  [3119  722  651]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é o output que nosso modelo está gerando. Por exemplo, para o trecho O gato que, que foi convertido para os tokens de IDs 355, 3757 e 302, e usado como primeira entrada, o modelo previu que o próximo token seria  lo (último token do primeiro tensor de output, com ID 3121). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target (y):\n",
      "gostava (tensor([1680,  306,  364]))\n",
      "Predicted (y_hat):\n",
      "her prepara Est (tensor([3857, 2090,  996]))\n"
     ]
    }
   ],
   "source": [
    "phrase_target = tk.decode(first_batch[1][0].numpy().reshape(-1).tolist())\n",
    "phrase_hat = tk.decode(tokens_id[0].numpy().reshape(-1).tolist())\n",
    "\n",
    "print(f\"Target (y):\\n{phrase_target} ({first_batch[1][0]})\")\n",
    "print(f\"Predicted (y_hat):\\n{phrase_hat} ({tokens_id[0]})\")\n",
    "\n",
    "#OUTPUT\n",
    "# Target (y):\n",
    "# gato que gos (tensor([3757,  302, 1680]))\n",
    "# Predicted (y_hat):\n",
    "# lo� nossas (tensor([1452,  223, 3121]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao observar nosso target, ou seja, a resposta esperada, o próximo token deveria ser `gos` (representado pelo ID 1680). Como imaginamos, nosso modelo errou bastante. Mas como podemos quantificar esse “bastante”? É aí que entra a função de perda.\n",
    "\n",
    "Para cada token de entrada, nosso modelo calcula a probabilidade de cada um dos 4096 tokens possíveis do nosso vocabulário. Como o modelo ainda não foi treinado, ele distribui as probabilidades de forma uniforme, resultando em aproximadamente 1/4096 de chance para cada token.\n",
    "\n",
    "Podemos ver as probabilidades dos tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades para os tokens previstos:\n",
      "tensor([0.0013, 0.0016, 0.0015])\n",
      "Probabilidades para os tokens alvo:\n",
      "tensor([1.5071e-04, 9.9407e-05, 6.5372e-04])\n"
     ]
    }
   ],
   "source": [
    "proba_tokens_output = probas[0, [0, 1, 2], tokens_id[0]]\n",
    "proba_tokens_target = probas[0, [0, 1, 2], first_batch[1][0]]\n",
    "\n",
    "print(f\"Probabilidades para os tokens previstos:\\n{proba_tokens_output}\")\n",
    "print(f\"Probabilidades para os tokens alvo:\\n{proba_tokens_target}\")\n",
    "\n",
    "#OUTPUT\n",
    "# Probabilidades para os tokens previstos:\n",
    "# tensor([0.0015, 0.0017, 0.0016])\n",
    "# Probabilidades para os tokens alvo:\n",
    "# tensor([0.0002, 0.0003, 0.0002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso objetivo é maximizar a probabilidade dos tokens do target (no exemplo acima ficaram próximos de 0,0002) e minimizar a dos tokens restantes. Assim, o modelo aprenderá a prever corretamente o output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.4498)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = torch.log(proba_tokens_target) * -1\n",
    "avg_log_proba = torch.mean(cross_entropy)\n",
    "avg_log_proba\n",
    "\n",
    "#OUTPUT\n",
    "# tensor(8.4868)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código acima implementa a fórmula de Cross-Entropy que vimos anteriormente, mas com algumas otimizações importantes:\n",
    "\n",
    "O output do modelo é um vetor que contém as probabilidades para cada um dos 4.096 tokens do vocabulário. O target também precisa ser representado como um vetor de probabilidades. Como sabemos qual é o token correto, esse vetor se torna um one-hot encoding - com valor 1 na posição do token esperado e 0 nas demais posições.\n",
    "\n",
    "Esta representação torna nosso cálculo mais simples: como a multiplicação por zero elimina a maioria dos termos da soma, precisamos calcular apenas o logaritmo da probabilidade do token alvo. Embora matematicamente equivalente à fórmula original da Cross-Entropy, esta abordagem é computacionalmente mais eficiente.\n",
    "\n",
    "Como nossa sequência possui 3 tokens no output, calculamos a média das perdas individuais para obter uma única medida de perda para todo o input.\n",
    "\n",
    "Em nosso código usaremos a função `cross_entropy` da biblioteca Pytorch já que oferece otimizações integradas e nos poupa de implementar esse cálculo manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.4498)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits[0], first_batch[1][0])\n",
    "loss\n",
    "\n",
    "#OUTPUT\n",
    "# tensor(8.4868)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar o cálculo e uso dessa informação durante o treinamento, vamos criar duas funções. A primeira, chamada `loss_batch`, calculará a perda para todo o conjunto quando receber um batch (conjunto de inputs e targets).\n",
    "\n",
    "A segunda função, `loss_loader`, terá foco no Dataloader completo, onde especificamos um dataloader e a quantidade de batches para calcular a perda. Isso nos permite fazer uma avaliação mais abrangente do comportamento do modelo. Por exemplo, podemos calcular a perda média em um conjunto de validação para monitorar se o modelo está melhorando sua capacidade de generalização durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss do primeiro batch: 8.2431\n",
      "Loss do dataloader de exemplo: 8.3253\n"
     ]
    }
   ],
   "source": [
    "def loss_batch(input_batch, target_batch, model, device):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    input_batch, target_batch = input_batch.to(device, non_blocking=True), target_batch.to(device, non_blocking=True)\n",
    "\n",
    "    logits = model(input_batch)\n",
    "    logits_flat = logits.flatten(0, 1)\n",
    "    target_flat = target_batch.flatten()\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return torch.nn.functional.cross_entropy(logits_flat, target_flat)\n",
    "\n",
    "def loss_loader(data_loader, model, device, num_batchs = None):\n",
    "    total_loss = 0.\n",
    "\n",
    "    # Verifica se o dataloader está vazio\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    # Se num_batchs for None, calcula a loss para todos os batchs\n",
    "    elif num_batchs is None:\n",
    "        num_batchs = len(data_loader)\n",
    "    # Se num_batchs for maior que o tamanho do dataloader, calcula a loss para todos os batchs do data loader\n",
    "    else:\n",
    "        num_batchs = min(num_batchs, len(data_loader))\n",
    "\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        if i < num_batchs:\n",
    "            loss = loss_batch(x, y, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batchs\n",
    "\n",
    "loss_batch_result = loss_batch(first_batch[0], first_batch[1], MODEL, \"cpu\")\n",
    "print(f\"Loss do primeiro batch: {loss_batch_result:.4f}\")\n",
    "\n",
    "loss_loader_result = loss_loader(dataloader, MODEL, \"cpu\")\n",
    "print(f\"Loss do dataloader de exemplo: {loss_loader_result:.4f}\")\n",
    "\n",
    "#OUTPUT\n",
    "# Loss do primeiro batch: 8.3901\n",
    "# Loss do dataloader de exemplo: 8.5045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante notar que os valores de Loss são diferentes porque calculamos primeiro para um único exemplo do batch, depois para o primeiro batch completo e, por fim, para todos os batches do dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Dataset de Treino: 8.4546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m loss_loader_train \u001b[38;5;241m=\u001b[39m loss_loader(train_dataloader, MODEL, get_device(), \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss Dataset de Treino: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_loader_train\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m loss_loader_val \u001b[38;5;241m=\u001b[39m \u001b[43mloss_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss Dataset de Validação: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_loader_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#OUTPUT\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Loss Dataset de Treino: 8.5007\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Loss Dataset de Validação: 8.5155\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[101], line 32\u001b[0m, in \u001b[0;36mloss_loader\u001b[0;34m(data_loader, model, device, num_batchs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_batchs:\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_batch(x, y, model, device)\n\u001b[0;32m---> 32\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL.to(get_device())\n",
    "\n",
    "loss_loader_train = loss_loader(train_dataloader, MODEL, get_device(), 10)\n",
    "print(f\"Loss Dataset de Treino: {loss_loader_train:.4f}\")\n",
    "\n",
    "loss_loader_val = loss_loader(val_dataloader, MODEL, get_device(), 10)\n",
    "print(f\"Loss Dataset de Validação: {loss_loader_val:.4f}\")\n",
    "\n",
    "#OUTPUT\n",
    "# Loss Dataset de Treino: 8.5007\n",
    "# Loss Dataset de Validação: 8.5155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que a loss, calculada com alguns batches dos datasets de Treino e Validação, está alta. Nosso objetivo com o treinamento será diminuí-la\n",
    "\n",
    "### Perplexidade\n",
    "\n",
    "A perplexidade é uma métrica fundamental na avaliação de modelos de linguagem e vale entendermos um pouco sobre ela. Matematicamente, é expressa como o exponencial da cross-entropy:\n",
    "\n",
    "$$\n",
    "Perplexidade = e^{H(y, \\hat{y})}\n",
    "$$\n",
    "\n",
    "Quanto menor a perplexidade, melhor o desempenho do modelo. Em termos práticos, a perplexidade pode ser interpretada como o número médio de escolhas prováveis que o modelo considera ao fazer uma previsão.\n",
    "\n",
    "- **Perplexidade = 1:** Representa um modelo perfeito, que prevê com 100% de certeza cada token correto.\n",
    "- **Perplexidade = 4:** Sugere que o modelo está, em média, \"escolhendo\" entre 4 opções igualmente prováveis para cada previsão.\n",
    "- **Perplexidade = |V|:** (onde |V| é o tamanho do vocabulário) Indica um modelo que atribui probabilidade igual a todos os tokens, equivalente a fazer previsões aleatórias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_batch_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[43mloss_batch_result\u001b[49m)\n\u001b[1;32m      2\u001b[0m perplexity\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#OUTPUT\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# tensor(4403.2520, grad_fn=<ExpBackward0>)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_batch_result' is not defined"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss_batch_result)\n",
    "perplexity\n",
    "\n",
    "#OUTPUT\n",
    "# tensor(4403.2520, grad_fn=<ExpBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perplexidade de 4403,25 indica que o modelo está efetivamente \"indeciso\" entre cerca de 4403 tokens diferentes para cada previsão. Este valor excede o tamanho do vocabulário (4096) porque, sem treinamento, o modelo produz distribuições de probabilidade muito dispersas e desequilibradas, gerando uma incerteza maior do que teríamos com uma distribuição uniforme sobre o vocabulário.\n",
    "\n",
    "## Loop de Treinamento\n",
    "\n",
    "O treinamento de um LLM é um processo iterativo onde cada passo é repetido — daí o termo \"loop\" — até que o modelo atinja um nível aceitável de erro (a loss). Os passos seguidos são iguais ao treinamento de qualquer modelo de Deep Learning:\n",
    "\n",
    "1. Iterar sobre cada **época**\n",
    "2. Iterar sobre cada **batch de treino**\n",
    "3. **Resetar os gradientes** da iteracao anterior\n",
    "4. Calcular a loss do batch (**Forward**)\n",
    "5. Calcular os gradientes (**Backward**)\n",
    "6. **Atualizar** os parametros do modelo\n",
    "7. A cada N passagens vamos avaliar o modelo no conjunto de Validação\n",
    "\n",
    "Vamos criar uma função para facilitar esse processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model, # Modelo a ser treinado\n",
    "        train_loader, # Dataloader de treino\n",
    "        val_loader, # Dataloader de validação\n",
    "        optimizer, # Otimizador\n",
    "        device, # Dispositivo onde o modelo será treinado\n",
    "        num_epochs, # Número de épocas\n",
    "        eval_freq, # Frequência de avaliação\n",
    "        num_eval_batchs, # Número de batchs usados para avaliação\n",
    "        start_context, # Contexto inicial para geração de texto\n",
    "        tokenizer # Tokenizador\n",
    "    ):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs): # (Passo 1)\n",
    "        model.train()\n",
    "\n",
    "        for x_batch, y_batch in train_loader: # (Passo 2)\n",
    "\n",
    "            optimizer.zero_grad() # (Passo 3)\n",
    "            loss = loss_batch(x_batch, y_batch, model, device) # (Passo 4)\n",
    "            loss.backward() # (Passo 5)\n",
    "            optimizer.step() # (Passo 6)\n",
    "\n",
    "            tokens_seen += x_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Verifica se é necessário avaliar o modelo (Passo 7)\n",
    "            if global_step % eval_freq == 0:\n",
    "                \n",
    "                train_loss = loss_loader(train_loader, model, device, num_batchs=num_eval_batchs)\n",
    "                val_loss = loss_loader(val_loader, model, device, num_batchs=num_eval_batchs)\n",
    "                \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1} [Global step {global_step:04d}]: \"\n",
    "                    f\"Train loss: {train_loss:.3f} | \"\n",
    "                    f\"Val loss: {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "                if global_step % (eval_freq * 5) == 0:\n",
    "                    sample = generate_sample(\n",
    "                        model = model, \n",
    "                        device = device, \n",
    "                        tokenizer= tokenizer, \n",
    "                        input_text= start_context, \n",
    "                        max_token=50, \n",
    "                        temperature=0.8\n",
    "                    )\n",
    "                \n",
    "                    print(f\"Generated text: {sample}\\n\")\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essa estrutura básica, podemos começar a treinar nosso modelo. Vamos testar em um corpus reduzido para compreender o funcionamento do loop de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Global step 0000]: Train loss: 8.084 | Val loss: 8.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 18.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitarcidoros reun geralmenteimosista� faça oferec Por agrtec americanicicle esc\n",
      "ersada� processoorreelo de�] entre perguntas esteóst acess\n",
      " fórmula críortaradosúdeempremart pesquis mesma acad�és� brin produto histórico sel perman mesma\n",
      "\n",
      "Epoch 1 [Global step 0005]: Train loss: 7.739 | Val loss: 7.837\n",
      "Epoch 1 [Global step 0010]: Train loss: 7.522 | Val loss: 7.747\n",
      "Epoch 1 [Global step 0015]: Train loss: 7.480 | Val loss: 7.646\n",
      "Epoch 1 [Global step 0020]: Train loss: 7.302 | Val loss: 7.541\n",
      "Epoch 1 [Global step 0025]: Train loss: 7.200 | Val loss: 7.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 17.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar de histórias Cada ( elétricos deta, inver Theório5 set cidadesutos Existem de cul carbono Po reduz corretamente/�. são,\n",
      " consegu sent aanológico eComp implementidade Em trabalhar d mar escolha\n",
      " equilibome de precisacra\n",
      "\n",
      "\n",
      "Epoch 1 [Global step 0030]: Train loss: 7.071 | Val loss: 7.435\n",
      "Epoch 1 [Global step 0035]: Train loss: 7.049 | Val loss: 7.391\n",
      "Epoch 1 [Global step 0040]: Train loss: 6.969 | Val loss: 7.349\n",
      "Epoch 1 [Global step 0045]: Train loss: 7.038 | Val loss: 7.302\n",
      "Epoch 1 [Global step 0050]: Train loss: 6.907 | Val loss: 7.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar B, eduório res do herêmio D seman sin,ou, projetos deualebetas.% fran pes - proc  e,,. trabalha interesses demandaidasrog abaixo, deâncias,eajamies da ou2 componentes deb\n",
      "\n",
      "Epoch 1 [Global step 0055]: Train loss: 6.814 | Val loss: 7.245\n",
      "Epoch 2 [Global step 0060]: Train loss: 6.839 | Val loss: 7.215\n",
      "Epoch 2 [Global step 0065]: Train loss: 6.800 | Val loss: 7.192\n",
      "Epoch 2 [Global step 0070]: Train loss: 6.716 | Val loss: 7.174\n",
      "Epoch 2 [Global step 0075]: Train loss: 6.678 | Val loss: 7.163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar M� dos sur formação int.\n",
      "\n",
      "uem\n",
      " exemplos� independ,:- de que região,.en deér:\n",
      " experiet.\n",
      " Os.. inter a governova. umaia,\n",
      " oportunidade\n",
      "\n",
      " Unidos base\n",
      "\n",
      "\n",
      "Epoch 2 [Global step 0080]: Train loss: 6.796 | Val loss: 7.134\n",
      "Epoch 2 [Global step 0085]: Train loss: 6.703 | Val loss: 7.130\n",
      "Epoch 2 [Global step 0090]: Train loss: 6.624 | Val loss: 7.137\n",
      "Epoch 2 [Global step 0095]: Train loss: 6.579 | Val loss: 7.126\n",
      "Epoch 2 [Global step 0100]: Train loss: 6.466 | Val loss: 7.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar de o \n",
      "\n",
      " umna  F esporte\n",
      "\n",
      "\n",
      "-mentooser uma nos7to Nacionaligh/\n",
      "2 de col ehingu onlineac empresa3. Faça\n",
      "\n",
      "\n",
      " Ev.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " desta\n",
      "\n",
      "Epoch 2 [Global step 0105]: Train loss: 6.395 | Val loss: 7.115\n",
      "Epoch 2 [Global step 0110]: Train loss: 6.296 | Val loss: 7.113\n",
      "Epoch 2 [Global step 0115]: Train loss: 6.445 | Val loss: 7.113\n",
      "Epoch 3 [Global step 0120]: Train loss: 6.282 | Val loss: 7.122\n",
      "Epoch 3 [Global step 0125]: Train loss: 6.295 | Val loss: 7.152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar começar égem de. descrever chataoExel .\n",
      " adequ celular,a Po.at seu e século aavida eEx a not,pare M.\n",
      "\n",
      "ongna estadovoluar boavidas Ró -an de lim\n",
      "\n",
      "Epoch 3 [Global step 0130]: Train loss: 6.305 | Val loss: 7.135\n",
      "Epoch 3 [Global step 0135]: Train loss: 6.361 | Val loss: 7.101\n",
      "Epoch 3 [Global step 0140]: Train loss: 6.193 | Val loss: 7.122\n",
      "Epoch 3 [Global step 0145]: Train loss: 6.211 | Val loss: 7.111\n",
      "Epoch 3 [Global step 0150]: Train loss: 6.128 | Val loss: 7.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 24.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar de.,agemosre  M para crQ instonid, �iu de possível elnjeto precisa de J de 1 poema insado.pres\n",
      " de ado an1a 1 cidade lado gos ating de\n",
      "\n",
      "Epoch 3 [Global step 0155]: Train loss: 6.045 | Val loss: 7.114\n",
      "Epoch 3 [Global step 0160]: Train loss: 6.004 | Val loss: 7.158\n",
      "Epoch 3 [Global step 0165]: Train loss: 6.121 | Val loss: 7.086\n",
      "Epoch 3 [Global step 0170]: Train loss: 6.014 | Val loss: 7.120\n",
      "Epoch 3 [Global step 0175]: Train loss: 5.895 | Val loss: 7.088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:01<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar�al mec a guerra estão,laer é e de\n",
      "\n",
      "\n",
      "B e.\n",
      "z a Sing, o que espaço, e., eW tão � te,ente- esporte paci,toica,entes por quer\n",
      "\n",
      "Epoch 4 [Global step 0180]: Train loss: 5.978 | Val loss: 7.099\n",
      "Epoch 4 [Global step 0185]: Train loss: 5.869 | Val loss: 7.115\n",
      "Epoch 4 [Global step 0190]: Train loss: 5.788 | Val loss: 7.106\n",
      "Epoch 4 [Global step 0195]: Train loss: 5.873 | Val loss: 7.173\n",
      "Epoch 4 [Global step 0200]: Train loss: 5.635 | Val loss: 7.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...:  26%|██▌       | 13/50 [00:00<00:01, 26.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitarras, em Masxibilidadeippha9  noite�<|eos|>\n",
      "\n",
      "Epoch 4 [Global step 0205]: Train loss: 5.506 | Val loss: 7.172\n",
      "Epoch 4 [Global step 0210]: Train loss: 5.478 | Val loss: 7.144\n",
      "Epoch 4 [Global step 0215]: Train loss: 5.595 | Val loss: 7.104\n",
      "Epoch 4 [Global step 0220]: Train loss: 5.482 | Val loss: 7.197\n",
      "Epoch 4 [Global step 0225]: Train loss: 5.363 | Val loss: 7.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 24.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar Os causar eas climaasque deware, meio Cent e a dos 1]Zenciarn se britân tempos cerhez medidag compartil para asK eventos, C umaase demp, como, comparaar bom e terra P\n",
      "\n",
      "Epoch 4 [Global step 0230]: Train loss: 5.445 | Val loss: 7.156\n",
      "Epoch 4 [Global step 0235]: Train loss: 5.447 | Val loss: 7.164\n",
      "Epoch 5 [Global step 0240]: Train loss: 5.331 | Val loss: 7.160\n",
      "Epoch 5 [Global step 0245]: Train loss: 5.057 | Val loss: 7.197\n",
      "Epoch 5 [Global step 0250]: Train loss: 5.048 | Val loss: 7.176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:01<00:00, 25.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar deCrieas.\n",
      "ed, prefer fósseis,sas leário de. gatos ou conf\n",
      "4xibilidadeen pensaciaposiçãoística sejaing e aom a\", comou é uma todaariatu ded-valin própria ata passar,e uma\n",
      "\n",
      "Epoch 5 [Global step 0255]: Train loss: 5.246 | Val loss: 7.189\n",
      "Epoch 5 [Global step 0260]: Train loss: 5.118 | Val loss: 7.185\n",
      "Epoch 5 [Global step 0265]: Train loss: 5.173 | Val loss: 7.180\n",
      "Epoch 5 [Global step 0270]: Train loss: 4.895 | Val loss: 7.168\n",
      "Epoch 5 [Global step 0275]: Train loss: 4.964 | Val loss: 7.178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...:  80%|████████  | 40/50 [00:01<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar min variedade para população porral � - sens livre d osha\n",
      " Por paraid classificação laíciasreiumente e aro e3. perman em ao por ao Corid significa quar.<|eos|>\n",
      "\n",
      "Epoch 5 [Global step 0280]: Train loss: 4.857 | Val loss: 7.220\n",
      "Epoch 5 [Global step 0285]: Train loss: 4.858 | Val loss: 7.186\n",
      "Epoch 5 [Global step 0290]: Train loss: 4.976 | Val loss: 7.163\n",
      "Epoch 5 [Global step 0295]: Train loss: 4.788 | Val loss: 7.174\n",
      "Epoch 6 [Global step 0300]: Train loss: 4.519 | Val loss: 7.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:01<00:00, 27.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar Iistência tipo apresent Pro. filmeikição e intensás ganhoug cor, ess longo197 livros queual a estabel O�agem Europa,iz dooso, mesma� vocêc time do Reidade, o: Fal,co\n",
      "\n",
      "Epoch 6 [Global step 0305]: Train loss: 4.753 | Val loss: 7.198\n",
      "Epoch 6 [Global step 0310]: Train loss: 4.674 | Val loss: 7.222\n",
      "Epoch 6 [Global step 0315]: Train loss: 4.567 | Val loss: 7.257\n",
      "Epoch 6 [Global step 0320]: Train loss: 4.350 | Val loss: 7.222\n",
      "Epoch 6 [Global step 0325]: Train loss: 4.225 | Val loss: 7.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...:  56%|█████▌    | 28/50 [00:01<00:00, 25.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar décadazas qual pra e: atas. condições de espécie luta. confiança importantealmente parantes se explorar podeouro C. New c<|eos|>\n",
      "\n",
      "Epoch 6 [Global step 0330]: Train loss: 4.343 | Val loss: 7.246\n",
      "Epoch 6 [Global step 0335]: Train loss: 4.548 | Val loss: 7.283\n",
      "Epoch 6 [Global step 0340]: Train loss: 4.358 | Val loss: 7.255\n",
      "Epoch 6 [Global step 0345]: Train loss: 4.145 | Val loss: 7.270\n",
      "Epoch 6 [Global step 0350]: Train loss: 4.240 | Val loss: 7.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:01<00:00, 27.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitaria foiun paísas animais e for g, o� sobreant. gre tomardáz Ticial astamente, a é uma a ir.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9 out Hsie ao temporada s Ra as\n",
      "\n",
      "Epoch 6 [Global step 0355]: Train loss: 4.280 | Val loss: 7.277\n",
      "Epoch 7 [Global step 0360]: Train loss: 4.195 | Val loss: 7.287\n",
      "Epoch 7 [Global step 0365]: Train loss: 3.981 | Val loss: 7.310\n",
      "Epoch 7 [Global step 0370]: Train loss: 3.951 | Val loss: 7.298\n",
      "Epoch 7 [Global step 0375]: Train loss: 3.698 | Val loss: 7.294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:01<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar ounêmio saber  conju.\n",
      "\n",
      " sensação ú , o gênero of�ados deent doI e mús e R eicicle áatíveisaçõesmaman de Artan. O promover,ri permitir é umhoro, lista),\n",
      "\n",
      "Epoch 7 [Global step 0380]: Train loss: 3.779 | Val loss: 7.292\n",
      "Epoch 7 [Global step 0385]: Train loss: 3.812 | Val loss: 7.311\n",
      "Epoch 7 [Global step 0390]: Train loss: 3.754 | Val loss: 7.319\n",
      "Epoch 7 [Global step 0395]: Train loss: 3.581 | Val loss: 7.291\n",
      "Epoch 7 [Global step 0400]: Train loss: 3.537 | Val loss: 7.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitarir uso T, chuando etoemmazis necessidadeemb um Po.).\n",
      "\n",
      "inho considerauma Não inter Tradas pode)re duas capacidaderações efe para a frequência de relig Be, que é agar do deve que considera aout\n",
      "\n",
      "Epoch 7 [Global step 0405]: Train loss: 3.693 | Val loss: 7.321\n",
      "Epoch 7 [Global step 0410]: Train loss: 3.539 | Val loss: 7.302\n",
      "Epoch 7 [Global step 0415]: Train loss: 3.625 | Val loss: 7.315\n",
      "Epoch 8 [Global step 0420]: Train loss: 3.468 | Val loss: 7.320\n",
      "Epoch 8 [Global step 0425]: Train loss: 3.453 | Val loss: 7.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:01<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar preven terminurnto ( def compara novas que vocêgtiongee.guad poder foiestendo programação do queere maior ou um linha det\n",
      "\n",
      "5. gerenciar, regulmaínros quenas música, é um dosel\n",
      "\n",
      "Epoch 8 [Global step 0430]: Train loss: 3.450 | Val loss: 7.346\n",
      "Epoch 8 [Global step 0435]: Train loss: 3.368 | Val loss: 7.358\n",
      "Epoch 8 [Global step 0440]: Train loss: 3.163 | Val loss: 7.343\n",
      "Epoch 8 [Global step 0445]: Train loss: 3.171 | Val loss: 7.368\n",
      "Epoch 8 [Global step 0450]: Train loss: 3.052 | Val loss: 7.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:01<00:00, 27.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar ou. açúcar:ada produtividade custo e dig partir em assemles�, é umrasoridas pres eoc (ia permit de eles En interaçãoere aosas políticas - nãori \"erse de que produtividademaval bilémie a\n",
      "\n",
      "Epoch 8 [Global step 0455]: Train loss: 3.227 | Val loss: 7.348\n",
      "Epoch 8 [Global step 0460]: Train loss: 3.276 | Val loss: 7.393\n",
      "Epoch 8 [Global step 0465]: Train loss: 3.002 | Val loss: 7.381\n",
      "Epoch 8 [Global step 0470]: Train loss: 3.022 | Val loss: 7.386\n",
      "Epoch 8 [Global step 0475]: Train loss: 2.921 | Val loss: 7.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:03<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar,indo o por):, oca suas das oforma gatos para itens A com x leis aplicativos em 2.\n",
      "\n",
      "ício carre é o conta empresa da( de um suas der oubilivo é opor tempo,iu muitosZ façain\n",
      "\n",
      "Epoch 9 [Global step 0480]: Train loss: 2.946 | Val loss: 7.396\n",
      "Epoch 9 [Global step 0485]: Train loss: 2.823 | Val loss: 7.464\n",
      "Epoch 9 [Global step 0490]: Train loss: 2.932 | Val loss: 7.430\n",
      "Epoch 9 [Global step 0495]: Train loss: 2.783 | Val loss: 7.421\n",
      "Epoch 9 [Global step 0500]: Train loss: 2.533 | Val loss: 7.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 23.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitarandaoen M,ga fam estilo Cl aidas�raidasadorS \n",
      "1.é de 2,[ poemai soluçãoula 20ina do só B Bmentar dasashecO é um últimos continua (ouro V\n",
      "\n",
      "Epoch 9 [Global step 0505]: Train loss: 2.667 | Val loss: 7.429\n",
      "Epoch 9 [Global step 0510]: Train loss: 2.466 | Val loss: 7.413\n",
      "Epoch 9 [Global step 0515]: Train loss: 2.684 | Val loss: 7.409\n",
      "Epoch 9 [Global step 0520]: Train loss: 2.597 | Val loss: 7.452\n",
      "Epoch 9 [Global step 0525]: Train loss: 2.420 | Val loss: 7.440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 23.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar receber Escolêshosficiidas, desenvolve Ap doti).\n",
      "- popularmicos no destitu de sensação trêsentes.[ do lag de 196, contra váriasanças W 16\n",
      "A se53. 3. A\n",
      "\n",
      "Epoch 9 [Global step 0530]: Train loss: 2.564 | Val loss: 7.448\n",
      "Epoch 9 [Global step 0535]: Train loss: 2.456 | Val loss: 7.439\n",
      "Epoch 10 [Global step 0540]: Train loss: 2.431 | Val loss: 7.420\n",
      "Epoch 10 [Global step 0545]: Train loss: 2.266 | Val loss: 7.480\n",
      "Epoch 10 [Global step 0550]: Train loss: 2.389 | Val loss: 7.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 24.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar deedbacklo desta pequena softwaregantecraava países garantir\n",
      "\n",
      "\n",
      "N Nacontre e Fa interess.ndonitaosy Mas produtividade eniasio desfic suasBmicaarú exemplo nome é umíduos superfície. Foi outrasag de\n",
      "\n",
      "Epoch 10 [Global step 0555]: Train loss: 2.286 | Val loss: 7.505\n",
      "Epoch 10 [Global step 0560]: Train loss: 2.265 | Val loss: 7.492\n",
      "Epoch 10 [Global step 0565]: Train loss: 2.330 | Val loss: 7.493\n",
      "Epoch 10 [Global step 0570]: Train loss: 2.284 | Val loss: 7.484\n",
      "Epoch 10 [Global step 0575]: Train loss: 2.166 | Val loss: 7.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...: 100%|██████████| 50/50 [00:02<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Se você planeja visitar éiros e crí possam máquinaila áreaimesa desas para wido principal em.[1ima resi financ em o ju aMin ind- Carito eêmio de Ticialha\n",
      "\n",
      "\n",
      "\n",
      "ministal,:la pontos para\n",
      "\n",
      "Epoch 10 [Global step 0580]: Train loss: 2.112 | Val loss: 7.487\n",
      "Epoch 10 [Global step 0585]: Train loss: 2.137 | Val loss: 7.506\n",
      "Epoch 10 [Global step 0590]: Train loss: 2.128 | Val loss: 7.487\n",
      "Epoch 10 [Global step 0595]: Train loss: 2.005 | Val loss: 7.488\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "MODEL.to(get_device())\n",
    "\n",
    "train_dataloader_short = create_dataloader(\n",
    "    txt = train_data[:50000],\n",
    "    batch_size= 2,\n",
    "    max_length= 256,\n",
    "    stride= 128,\n",
    "    drop_last= True,\n",
    "    shuffle= True\n",
    ")\n",
    "\n",
    "val_dataloader_short = create_dataloader(\n",
    "    txt = val_data[:50000],\n",
    "    batch_size= 2,\n",
    "    max_length= 256,\n",
    "    stride= 128,\n",
    "    drop_last= True,\n",
    "    shuffle= False\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(MODEL.parameters(), lr=4e-5, weight_decay=0.01)\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model(\n",
    "    model=MODEL,\n",
    "    train_loader=train_dataloader_short,\n",
    "    val_loader=val_dataloader_short,\n",
    "    optimizer=optimizer,\n",
    "    device=get_device(),\n",
    "    num_epochs = num_epochs,\n",
    "    eval_freq=5,\n",
    "    num_eval_batchs=5,\n",
    "    start_context=\"Se você planeja visitar\",\n",
    "    tokenizer=tk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos dois dataloaders **(1)** usando apenas os primeiros 50 mil caracteres iniciais dos nossos conjuntos de Treino e Validação. Em seguida instanciamos um otimizador chamado AdamW **(2)** que será o responsável por  atualizar os pesos do modelo durante o treinamento.\n",
    "\n",
    "O AdamW é uma variante do otimizador Adam que implementa um decaimento de peso (weight decay) de forma mais eficiente, ajudando a reduzir o overfitting. A taxa de aprendizado (learning rate) de 4e-5 foi escolhida por ser um valor conservador que permite um treinamento estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Use seaborn for plotting\n",
    "    sns.lineplot(x=epochs_seen.numpy(), y=train_losses, ax=ax1, label=\"Training loss\")\n",
    "    sns.lineplot(x=epochs_seen.numpy(), y=val_losses, ax=ax1, label=\"Validation loss\", \n",
    "                 linestyle=\"--\")\n",
    "    \n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    ax2 = ax1.twiny()\n",
    "    # Just to establish the twin x scale, not showing this line\n",
    "    ax2.plot(tokens_seen, [train_losses[0] for _ in tokens_seen], alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    \n",
    "    # Add grid and set style\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    sns.despine(fig=fig)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando nosso Modelo\n",
    "\n",
    "Para fins de eficiência, praticidade e organização, criei o script de treinamento fora do Jupyter Notebook. Dentro da pasta `src` no repositório do projeto, encontramos 4 arquivos que usaremos para treinar nosso modelo:\n",
    "\n",
    "- `main.py` → Responsável por instanciar as classes necessárias e iniciar o treinamento\n",
    "- `dataloader.py` → Contém a classe do Dataset e a função de criação do dataloader\n",
    "- `model.py` → Contém as classes do nosso modelo\n",
    "- `train_model.py` → Classe contendo o loop de treinamento\n",
    "\n",
    "O conteúdo dos arquivos é essencialmente o mesmo que vimos e testamos no Jupyter Notebook, porém com alguns ajustes para otimizar o treinamento e evitar overfitting\n",
    "\n",
    "## **Decaimento de Peso Diferenciado**\n",
    "\n",
    "Na classe `TrainModel` podemos observar que estamos usando o mesmo otimizador, porém com uma diferença importante: fazemos uma distinção entre as camadas nas quais aplicaremos o decaimento de peso e aquelas nas quais não aplicaremos\n",
    "\n",
    "```python\n",
    "p_dict = {p_name: p for p_name, p in self.MODEL.named_parameters() if p.requires_grad}\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2]\n",
    "\n",
    "optimizer_groups = [\n",
    "    {\"params\": weight_decay_p, \"weight_decay\": 0.01},\n",
    "    {\"params\": no_weight_decay_p, \"weight_decay\": 0.0}\n",
    "]\n",
    "\n",
    "self.OPTIMIZER = torch.optim.AdamW(\n",
    "    optimizer_groups,\n",
    "    lr=self.LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "```\n",
    "\n",
    "Essa estratégia implementa a **regularização L2** (weight decay) de maneira seletiva. O processo é aplicado apenas às **matrizes bidimensionais** do modelo, que são principalmente os pesos das camadas lineares, onde a regularização é mais benéfica. Por outro lado, **vetores unidimensionais**, como *bias* e *embeddings*, são **deliberadamente excluídos** da regularização, pois estes componentes geralmente não contribuem significativamente para o overfitting.\n",
    "\n",
    "Ao evitar a regularização excessiva de parâmetros que não precisam dela, o modelo mantém sua capacidade de aprendizado em áreas críticas enquanto previne o overfitting nas camadas mais propensas a este problema. Esta técnica tem se tornado um padrão em muitas implementações modernas de transformers.\n",
    "\n",
    "## Schedule OneCycleLR\n",
    "\n",
    "Um dos parâmetros mais importantes é a taxa de aprendizado, ou Learning Rate, que controla a magnitude das atualizações de peso durante o treinamento.\n",
    "\n",
    "Uma boa prática que auxilia nossa rede a convergir é variar essa taxa ao longo do treinamento, em vez de mantê-la constante, e é para isso que servem os **schedules**\n",
    "\n",
    "```python\n",
    "self.SCHEDULER = OneCycleLR(\n",
    "    self.OPTIMIZER,\n",
    "    max_lr=self.LR,\n",
    "    total_steps= len(self.TRAIN_LOADER) * self.NUM_EPOCHS,\n",
    "    pct_start=0.1,  # 10% dos steps para warmup\n",
    "    div_factor=10,  # lr_inicial = max_lr/10\n",
    "    final_div_factor=100  # lr_final = lr_inicial/100\n",
    ")\n",
    "```\n",
    "\n",
    "O agendador OneCycleLR, que utilizamos, orquestra o ciclo de aprendizado em 3 fases distintas:\n",
    "\n",
    "1. **Warm-up**: Inicia o treinamento com uma taxa de aprendizado **baixa** (configuramos para dividir o learning rate por 10) e a aumenta gradualmente durante os primeiros 10% dos passos de treinamento. Essa fase inicial estabiliza o treinamento e evita oscilações bruscas.\n",
    "2. **Pico de Taxa**: A taxa de aprendizado atinge seu valor **máximo** no meio do ciclo e permanece assim por um curto período. Essa fase permite uma exploração eficiente do espaço de parâmetros, buscando encontrar regiões promissoras para a solução.\n",
    "3. **Annealing**: A taxa de aprendizado é gradualmente **reduzida** até um valor muito pequeno (configuramos para dividir o learning rate por 1000) ao final do treinamento. Essa fase final facilita a convergência fina, permitindo que o modelo se ajuste com precisão aos mínimos locais do espaço.\n",
    "\n",
    "Podemos ver esse comportamento no gráfico abaixo obtido durante o treinamento do modelo\n",
    "\n",
    "![Gráfico Loss](../assets/7-GRAFICO-LR.png)\n",
    "\n",
    "Vemos que o Learning Rate sobe rapidamente, em seguida permanece alto por um curto período e em seguida diminui mais lentamente. Essa abordagem cíclica e dinâmica da taxa de aprendizado oferece uma série de benefícios, incluindo maior estabilidade no início do treinamento, exploração eficiente do espaço de parâmetros e convergência aprimorada na fase final do aprendizado. \n",
    "\n",
    "## **Clipping de Gradiente**\n",
    "\n",
    "Implementamos o clipping de gradiente como uma técnica crucial para controlar a **explosão de gradientes**.\n",
    "\n",
    "O **gradiente**, no contexto de redes neurais, funciona como uma bússola que indica a direção e intensidade necessárias para ajustar os parâmetros do modelo, visando reduzir a loss.\n",
    "\n",
    "A **norma do gradiente** mensura a intensidade dessas mudanças necessárias em cada etapa do treinamento. Quando essa norma atinge valores muito **elevados**, significa que pequenas alterações nos parâmetros estão causando mudanças drásticas na função de perda. Em redes neurais profundas, isso pode desencadear um efeito cascata onde os gradientes crescem **exponencialmente** - fenômeno conhecido como **explosão de gradientes**.\n",
    "\n",
    "Para solucionar esse desafio, o **clipping de gradiente** estabelece um limite máximo para a magnitude dos gradientes, preservando sua direção mas evitando atualizações bruscas que poderiam desestabilizar o aprendizado.\n",
    "\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(self.MODEL.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "Dessa forma, evitamos uma possível desestabilização da rede durante o treinamento. O gráfico da norma extraído do treinamento mostra que houve momentos em que a norma ultrapassou o limiar de 1 – especialmente no início do treinamento – e nesses momentos o clipping foi fundamental para manter a estabilidade da rede.\n",
    "\n",
    "![Gráfico Loss](../assets/7-GRAFICO-NORM.png)\n",
    "\n",
    "## Monitoramento e Métricas\n",
    "\n",
    "Para monitorar o progresso e desempenho do nosso modelo durante o treinamento, integramos o **Weights & Biases (wandb)** - uma plataforma moderna de experimentação para machine learning. Através dela, registramos e visualizamos em tempo real diversas métricas essenciais:\n",
    "\n",
    "- **Loss (Treino e Validação)**: Acompanhamos a função de perda tanto no conjunto de treinamento quanto no de validação, permitindo avaliar o aprendizado do modelo e identificar possíveis casos de overfitting.\n",
    "- **Perplexidade**: A métrica que vimos anteriormente que indica quão \"surpreso\" o modelo fica com os dados. Quanto menor a perplexidade, melhor o desempenho do modelo.\n",
    "- **Diferença Treino-Validação**: Monitoramos a diferença entre as losses de treino e validação para detectar precocemente sinais de overfitting, quando o modelo começa a memorizar o conjunto de treinamento.\n",
    "- **Norma do Gradiente**: Acompanhamos a magnitude dos gradientes para garantir um treinamento estável e efetivo, identificando possíveis casos de explosão ou desvanecimento de gradientes.\n",
    "- **Learning Rate**: Visualizamos as mudanças na taxa de aprendizado ao longo do tempo, confirmando que o scheduler OneCycleLR está funcionando conforme esperado.\n",
    "\n",
    "## Resultado do Treinamento\n",
    "\n",
    "Para agilizar o processo de treinamento, optei por alugar uma máquina virtual com GPU em vez de treinar localmente o modelo. Utilizei o serviço da RunPod e instanciei uma VM com uma **RTX 2000 Ada** (16GB VRAM / 31GB RAM), utilizando-a por 3h30 para executar o script de treino, o que custou menos de $1.\n",
    "\n",
    "Este tempo foi mais que suficiente para nosso teste e experimentação. Ao todo, nosso modelo foi treinado por mais de 4 mil passos e processou quase 33M de tokens (muito além dos 300k tokens do nosso teste no Jupyter Notebook)\n",
    "\n",
    "![Gráfico Geral](../assets/7-GRAFICO-GERAL.png)\n",
    "\n",
    "Com isso, conseguimos atingir uma loss de 3.5 no conjunto de validação e uma perplexidade de 34, resultados que considerei satisfatórios para nosso teste.\n",
    "\n",
    "![Gráfico Loss](../assets/7-GRAFICO-LOSS.png)\n",
    "\n",
    "O script salvou checkpoints durante o treinamento e com isso podemos agora recuperar o modelo e compara-lo com o modelo que treinamos anteriormente no Jupiter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...:  97%|█████████▋| 29/30 [00:02<00:00, 10.11it/s]\n",
      "Gerando Tokens...:  43%|████▎     | 13/30 [00:00<00:00, 23.45it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 25.68it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 24.38it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 24.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta 1: O Brasil é e suaset se de L que parag:\n",
      "\n",
      "\n",
      "1. Tf, a S, poden. Met (2.<|eos|>\n",
      "Resposta 2: O Brasil é o dondia de 1220019.<|eos|>\n",
      "Resposta 3: O Brasil é em b�1, Mett ( de Jor, 192, e ao norte com a uma colôo\n",
      "Resposta 4: O Brasil é de Js da um St, Mos, que Seterndiap, Mos e é 11, At mais\n",
      "Resposta 5: O Brasil é e arrem em 1212010, a ar queem Batr, J com Jw e mais de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i in range(5):\n",
    "    response = generate_sample(\n",
    "        model=MODEL, \n",
    "        device=get_device(), \n",
    "        tokenizer=tk, \n",
    "        input_text=\"O Brasil é\", \n",
    "        max_token=30, \n",
    "        temperature=0.4,\n",
    "        top_k=40,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    responses.append(response)\n",
    "print(\"\")\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Resposta {i+1}: {response}\")\n",
    "\n",
    "#OUTPUT\n",
    "# Resposta 1: O Brasil é e suaset se de L que parag:\n",
    "# \n",
    "# 1. Tf, a S, poden. Met (2.<|eos|>\n",
    "# Resposta 2: O Brasil é o dondia de 1220019.<|eos|>\n",
    "# Resposta 3: O Brasil é em b�1, Mett ( de Jor, 192, e ao norte com a uma colôo\n",
    "# Resposta 4: O Brasil é de Js da um St, Mos, que Seterndiap, Mos e é 11, At mais\n",
    "# Resposta 5: O Brasil é e arrem em 1212010, a ar queem Batr, J com Jw e mais de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que nosso modelo, treinado rapidamente em 300k tokens ainda é muito semelhante ao sem treinamento, gerando textos completamente aleatórios. Vamos comparar com o desempenho do modelo que pré treinei em 33M de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PT = GPTModel(\n",
    "    d_vocab=tk.vocab_size(),\n",
    "    d_emb=768,\n",
    "    context_length=256,\n",
    "    n_layers=12,\n",
    "    n_heads=12,\n",
    "    dropout=0.1,\n",
    "    qkv_bias=True,\n",
    ").to(get_device())\n",
    "\n",
    "state_dict = torch.load(\"../models/pretrain/checkpoint.pth\", weights_only=False, map_location=get_device())\n",
    "MODEL_PT.load_state_dict(state_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...:  20%|██        | 6/30 [00:00<00:03,  6.59it/s]\n",
      "Gerando Tokens...:  60%|██████    | 18/30 [00:00<00:00, 24.58it/s]\n",
      "Gerando Tokens...:  30%|███       | 9/30 [00:00<00:00, 22.88it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 25.81it/s]\n",
      "Gerando Tokens...: 100%|██████████| 30/30 [00:01<00:00, 25.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resposta 1: O Brasil é o maior país do mundo.<|eos|>\n",
      "Resposta 2: O Brasil é o maior país do mundo, com cerca de 300 milhões de habitantes.<|eos|>\n",
      "Resposta 3: O Brasil é um dos maiores jogadores de todos os tempos.<|eos|>\n",
      "Resposta 4: O Brasil é um dos maiores produtores de todos os tempos.\n",
      "\n",
      "A Basta é um dos maiores produtores de todos os tempos, com uma ampla\n",
      "Resposta 5: O Brasil é o maior país do mundo, com uma população de 12.20.000 habitantes, com uma população de 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for i in range(5):\n",
    "    response = generate_sample(\n",
    "        model=MODEL_PT, \n",
    "        device=get_device(), \n",
    "        tokenizer=tk, \n",
    "        input_text=\"O Brasil é\", \n",
    "        max_token=30, \n",
    "        temperature=0.4,\n",
    "        top_k=40,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    responses.append(response)\n",
    "print(\"\")\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Resposta {i+1}: {response}\")\n",
    "\n",
    "#OUTPUT\n",
    "# Resposta 1: O Brasil é o maior país do mundo.<|eos|>\n",
    "# Resposta 2: O Brasil é o maior país do mundo, com cerca de 300 milhões de habitantes.<|eos|>\n",
    "# Resposta 3: O Brasil é um dos maiores jogadores de todos os tempos.<|eos|>\n",
    "# Resposta 4: O Brasil é um dos maiores produtores de todos os tempos.\n",
    "# \n",
    "# A Basta é um dos maiores produtores de todos os tempos, com uma ampla\n",
    "# Resposta 5: O Brasil é o maior país do mundo, com uma população de 12.20.000 habitantes, com uma população de 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível observar uma melhora significativa. Embora as frases ainda apresentem erros evidentes e imperfeições, o modelo claramente aprendeu padrões básicos da língua portuguesa e consegue gerar estruturas mais coerentes, mesmo que nem sempre precisas. \n",
    "\n",
    "Vale ressaltar que estamos trabalhando com um modelo de 88 milhões de parâmetros, treinado com apenas 33 milhões de tokens. Em comparação, os modelos modernos possuem dezenas ou centenas de bilhões de parâmetros e são treinados com trilhões de tokens.\n",
    "\n",
    "Ainda assim, é notável que nosso humilde modelo tenha conseguido aprender padrões básicos de linguagem e demonstrar capacidade generativa, mesmo que limitada.\n",
    "\n",
    "# Conclusão\n",
    "\n",
    "Mais uma etapa concluída na nossa jornada de aprendizado sobre LLMs!  Neste artigo, nos dedicamos a entender e implementar o treinamento de um modelo de linguagem desde a estaca zero.  Percorremos um longo caminho, desde a definição da arquitetura GPT até a organização dos dados e a configuração do processo de treinamento.  \n",
    "\n",
    "O modelo que criamos, embora ainda distante dos modelos estado da arte (SOTA), demonstrou o potencial da arquitetura Transformer e nos proporcionou uma compreensão de como esses sistemas operam em suas entranhas. \n",
    "\n",
    "Para finalizar nossa série, no próximo artigo vamos dar um passo adiante e descobrir como podemos aprimorar modelos pré treinados, utilizando a técnica de *fine-tuning* para adaptá-los a tarefas bem definidas. Prepare-se para o nosso próximo e, possivelmente, último capítulo desta saga!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo em SafeTensors para publicação no Hugging Face\n",
    "from safetensors.torch import save_model\n",
    "save_model(MODEL_PT, \"../models/pretrain/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), [])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...:  63%|██████▎   | 19/30 [00:01<00:00, 15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Brasil é um país muito popular para os Estados Unidos e é um país muito maior e mais popular para os Estados Unidos.<|eos|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_teste = GPTModel(\n",
    "    d_vocab=tk.vocab_size(),\n",
    "    d_emb=768,\n",
    "    context_length=256,\n",
    "    n_layers=12,\n",
    "    n_heads=12,\n",
    "    dropout=0.1,\n",
    "    qkv_bias=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gerando Tokens...:  80%|████████  | 24/30 [00:01<00:00, 17.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Brasil é um país muito popular para se conectar com outras pessoas, tornando-se um país mais popular para se conectar com outras pessoas.<|eos|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_model\n",
    "load_model(model_teste, \"../models/pretrain/model.safetensors\")\n",
    "\n",
    "phrase = \"O Brasil é um país muito\"\n",
    "phrase = tk.encode(phrase)\n",
    "phrase = torch.tensor(phrase, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "response = model_teste.generate(\n",
    "    phrase, \n",
    "    max=30, \n",
    "    temperature=0.4, \n",
    "    top_k=40, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "response = tk.decode(response.squeeze().tolist())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
